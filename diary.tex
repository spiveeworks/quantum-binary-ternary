\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\renewcommand\thesection{}
\renewcommand\thesubsection{}

\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}

\newcommand{\ord}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}

% Title Page
\title{Honours Diary 2020}
\author{Jarvis Carroll}


\begin{document}
\maketitle

\section{Notation}

In this diary unless explicitly stated within a section, I have been using the notation specified by Nielsen and Chuang, with the following additions:
\begin{itemize}
	\item Implicit quantifiers for index variables such as $i, j, k$. (Nielson and Chuang seem to do this actually, perhaps dropping more than I do)
	\begin{itemize}
		\item $\{x_i\} = \{x_i\ |\ i \in I\}$, $\{\ket{x_i}\} = \{\ket{x_i}\ |\ i \in I\}$ etc.
		\item $(x_i) = (x_1, x_2, \ldots, x_n)$
		\item $\sum_i$ in place of $\sum_{i \in I}$
		\item $\forall i$ in place of $\forall i \in I$
	\end{itemize}
\end{itemize}

\section{March 13}

Set up TeXstudio and basic document structure.

\subsection{Exercise 2.1}
Linear Dependence, show that $(1,-1)$, $(1,2)$ and $(2,1)$ are linearly dependent.

\begin{align*}
	&(1, -1) + (1, 2) - (2, 1) \\
=\ &(1+1-2, -1+2-1) \\
=\ &(0, 0)
\end{align*}

\subsection{Exercise 2.2}
Matrix representations: Suppose $V$ is a vector space with basis vectors
$\ket{0}$ and $\ket{1}$, and $A$ is a linear operator from $V$ to $V$ such that
$A\ket{0}=\ket{1}$ and $A\ket{1}=\ket{0}$. Give a matrix representation for
$A$, with respect to the input basis $\ket{0}$, $\ket{1}$, and the output basis
$\ket{0}$, $\ket{1}$. Find input and output bases which give rise to a different matrix representation of $A$.

Equation 2.12 gives us the defining property of matrix representations:
\[A\ket{v_j}=\sum_i A_{ij}\ket{w_i}\]
This gives us a pair of vector equations:
\[
	\ket{1} = A\ket{0} = A_{00}\ket{0} + A_{10}\ket{1}
\]
\[
	\ket{0} = A\ket{1} = A_{01}\ket{0} + A_{11}\ket{1}
\]
By linear independence of $\ket{0}$, $\ket{1}$, it follows that
\begin{align*}
A_{00} &= 0 & A_{01} &= 1 \\
A_{10} &= 1 & A_{11} &= 0
\end{align*}
i.e. A has the matrix representation:
\[
A = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
\]

\section{March 16}

\subsection{Exercise 2.3}
\[
A: V \to W
\]\[
B: W \to X
\]\[
	V = span\{\ket{v_i}\}\ \textit{etc.}
\]

At this point we are already identifying $A$, $B$ with their matrix representations. We would like to show equality between the function composition $B \circ A$ and the matrix product of the corresponding matrix representations, $B \times A$.
Once we have done this we will be able to identify both of these concepts as simply the expression $BA$, but for now we will use the explicit operators
% weird $\text{}$ layering to avoid soft-wrap putting \times on its own line
$\circ\ \text{and}\ \times$.

Our goal then is to show that the matrix representation of $B \circ A$ is $B \times A$.

\begin{align*}
	&\sum_i {(B \circ A)}_{ij} \ket{x_i} &\text{(arbitrary column of matrix $B \circ A$)}\\
	=\ &(B \circ A)\ket{v_j} &\text{(Definition of matrix representation)}\\
	=\ &B(A\ket{v_j}) &\text{(Definition of composition)}\\
	=\ &B\left(\sum_k A_{kj} \ket{w_k}\right) &\text{(Matrix representation)}\\
	=\ &\sum_k A_{kj} (B \ket{w_k}) &\text{(Linearity of B)}\\
	=\ &\sum_k A_{kj} \left( \sum_i B_{ik} \ket{x_i} \right) &\text{(Matrix representation)}\\
	=\ &\sum_i \left(\sum_k B_{ik} A_{kj}\right) \ket{x_i} &\text{(distribution)}\\
	=\ &\sum_i {(B \times A)}_{ij} \ket{x_i} &\text{(matrix product)}
\end{align*}
So by linear independence of $\ket{x_i}$ we know that ${(B \circ A)}_{ij} = {(B \times A)}_{ij}$ for arbitrary $i, j$, i.e.\ the matrix representation of $B \circ A$ is the matrix product $B \times A$.

\subsection{Exercise 2.4}

\[I: V \to V\]
\[I\ket{x_i} = \ket{x_i}\]

We would like to show that $I_{ij} = \delta_{ij}$.

\begin{align*}
	&\sum_i I_{ij} \ket{x_i} \\
	= &I\ket{x_j} \\
	= &\ket{x_j} \\
	= &\sum_i \delta_{ij}\ket{x_i}
\end{align*}

Again by linear independence of $\{\ket{x_i}\}$ we have $I_{ij} = \delta_{ij}$.

\subsection{Exercise 2.5}

\[((y_i), (z_i)) = \sum_i y_i^*z_i\]

We need to prove 3 properties. First linearity, taking $\ket{v} = (v_j) = (v_1,
v_2, \ldots, v_n)$ and $\ket{w_i} = (w_{ij}) = (w_{i1}, w_{i2}, \ldots, w_{in})$.

For this we define
\[\ket{z} = (z_j) = \sum_i\lambda_i\ket{w_i}\]
Then by the definitions of sum and scalar product in $\mathbb{C}^n$ we observe
\begin{align*}
&z_j \\
	=\ &{\left(\sum_i\lambda_i\ket{w_i}\right)}_j \\
	=\ &\sum_i{\left(\lambda_i\ket{w_i}\right)}_j \\
	=\ &\sum_i\lambda_i w_{ij}
\end{align*}
With this linearity falls out.
\begin{align*}
	&\left(\ket{v}, \ket{z}\right) \\
=\ &\left((v_j), (z_j)\right) \\
=\ &\sum_j y_j^*z_j \\
=\ &\sum_j y_j^*\left(\sum_i\lambda_i w_{ij}\right) \\
=\ &\sum_i\lambda_i\left(\sum_j v_j^*w_{ij}\right) \\
=\ &\sum_i\lambda_i\left((v_j), (w_{ij})\right) \\
	=\ &\sum_i\lambda_i\left(\ket{v}, \ket{w_i}\right)
\end{align*}

Next we prove conjugate symmetry.
\begin{align*}
	&{(\ket{w}, \ket{v})}^* \\
	=\ &{((w_i), (v_i))}^* \\
	=\ &{\left(\sum_i w_i^* v_i\right)}^* \\
	=\ &\sum_i w_i^{**}v_i^* \\
	=\ &\sum_i v_i^*w_i \\
	=\ &((v_i), (w_i)) \\
	=\ &(\ket{v}, \ket{w})
\end{align*}

Finally positivity:
\begin{align*}
	&(\ket{v}, \ket{v}) \\
=\ &((v_i), (v_i)) \\
=\ &\sum_i v_i^*v_i \\
=\ &\sum_i|v_i|^2
\end{align*}
Clearly this expression is at least 0, with equality when $v_i = 0\ \forall i$,
i.e.\ when $\ket{v} = (0, 0, \ldots, 0)$.

Therefore the operator $(\cdot, \cdot)$ is an inner product on the vector space $\mathbb{C}^n$.

\section{March 23}

\subsection{Exercise 2.6}

Combines second argument linearity with conjugate symmetry, as you would expect.
\begin{align*}
	\left(\sum_i\lambda_i\ket{w_i}, \ket{v}\right)
	&= {\left(\ket{v}, \sum_i\lambda_i\ket{w_i}\right)}^* \\
	&= {\left(\sum_i\lambda_i\left(\ket{v}, \ket{w_i}\right)\right)}^* \\
	&= \sum_i\lambda_i^*{\left(\ket{v}, \ket{w_i}\right)}^* \\
	&= \sum_i\lambda_i^*{\left(\ket{w_i}, \ket{v}\right)}
\end{align*}

\subsection{Exercise 2.7}

\[\braket{w}{v} = (1)(1)+(1)(-1) = 0\]
\[\ket{w}, \ket{v} = \left(\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}}\right)\]

\subsection{Exercise 2.8}

Suppose that $j < k$, and that the first $k-1$ vectors are orthonormal,
\begin{align*}
	\braket{v_j}{v_k}
	&= \braket{v_j}{w_k} - \sum_i^{k-1}\braket{v_i}{w_k}\braket{v_j}{v_i} \\
	&= \braket{v_j}{w_k} - \braket{v_j}{w_k} \braket{v_j}{v_j} \\
	&= \braket{v_j}{w_k} - \braket{v_j }{ w_k} \braket{v_j}{v_j} \\
&= 0
\end{align*}

Obviously $\ket{v_i}$ are explicitly normalized so we are done.

\section{March 27}

Feel like generalizing Ex 2.9 based on Ex 2.10.

\subsection{Exercise 2.10}

Suppose $\ket{v_i}$ is an orthonormal basis for an inner product space
$V$.What is the matrix representation for the operator
$\ket{v_j}\bra{v_k}$, with respect to the $\ket{v_i}$ basis?

Let $A = \ket{v_j}\bra{v_k}$, and $A\ket{v_n} = A_{mn}\ket{v_m}$, then
\begin{align*}
	A_{mn}\ket{v_m}
	&= \ket{v_j} \braket{v_k}{v_n} \\
	&= \delta_{kn}\ket{v_j} \\
\implies A_{mn}
&= \begin{cases}
\delta_{kn} & m = j \\
0 & else
\end{cases} \\
&= \begin{cases}
1 & m = j, n = k \\
0 & else
\end{cases}
\end{align*}

e.g. in a 2d space,
\[\ket{0}\bra{1} = \left[\begin{matrix}
0 & 1 \\
0 & 0
\end{matrix}\right]\]

\subsection{Exercise 2.9}

From 2.10 it becomes clear that
\[
I = \left[\begin{matrix}
1 & 0 \\
0 & 1
\end{matrix}\right] = \ket{0}\bra{0} + \ket{1}\bra{1}
\]
\[
X = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right] = \ket{0}\bra{1} + \ket{1}\bra{0}
\]
\[
Y = \left[\begin{matrix}
0 & -i \\
i & 0
\end{matrix}\right] = i\ket{1}\bra{0} -i\ket{0}\bra{1}
\]
\[
Z = \left[\begin{matrix}
1 & 0 \\
0 & -1
\end{matrix}\right] = \ket{0}\bra{0} - \ket{1}\bra{1}
\]

\subsection{Matrix Reps and Outer Products}

Visually one can see that we can use $\ket{v_i} \bra{v_j}$ as a basis for describing linear maps directly in terms of their matrix representation, e.g.

\[\left[\begin{matrix}a&b\\c&d\end{matrix}\right] = a\ket{0}\bra{0} +
	b\ket{0}\bra{1} + c\ket{1}\bra{0} + d\ket{1}\bra{1}\]

Stated more generally

\[A = \sum_{ij} A_{ij}\ket{v_i}\bra{v_j}\]

Which connects to equation 2.25 via the equality

\[\bra{v_j} A \ket{ v_i} = A_{ij}\]

Each of these things can be shown, especially once linear maps are themselves understood as a vector space, but I shall take these statements as given.

\subsection{Exercise 2.11}
Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.

Z is already diagonal.

X has the familiar and intuitive eigenvectors $\ket{+}$ and $\ket{-}$:
\begin{align*}
	X\ket{+} = \ket{+} \\
	X\ket{-} = -\ket{-}
\end{align*}
Diagonal representation then is $X = \ket{+}\bra{+}\ -\ \ket{-}\bra{-}$

$Y$ has the same shape as $X$ so we could guess the eigenvectors are the same,
but $Y\ket{+} = -i\ket{-}$.

Solving properly we get $c(\lambda) = \lambda^2 - 1 = 0$, with eigenvectors in the null-spaces of:
\[
\left[\begin{matrix}-1&-i\\i&-1\end{matrix}\right]
\left[\begin{matrix}a\\b\end{matrix}\right]
=\left[\begin{matrix}0\\0\end{matrix}
\right]
\]
Second row is $-i$ times first row:
\[Y(\ket{0} -i \ket{1}) = -\ket{0} +i\ket{1}\]
Taking the conjugate we unsurprisingly get
\[Y(\ket{0} +i \ket{1}) = \ket{0} +i\ket{1}\]
Their diagonal representation would be just what you'd expect,
$\ket{v_0}\bra{v_0} - \ket{v_1}\bra{v_1}$ where $\ket{v_0} =
2^{-\frac{1}{2}}\left(\ket{0} + i\ket{1}\right)$, $\ket{v_1}
= 2^{-\frac{1}{2}}\left(\ket{0} - i\ket{1}\right)$.

\section{March 30}
\subsection{Exercise 2.12}
Prove that the matrix
\[\left[\begin{matrix}1&1\\1&0\end{matrix}\right]\]
is not diagonalizable.

The equation $c(\lambda) = (1-\lambda)^2 = 0$ has repeated root $\lambda = 1$, so it either has one eigenvector, or a eigenspace of 2 degenerate eigenvectors.

Clearly if it had 2 degenerate eigenvectors it would simply be the identity operation, mapping all vectors to themselves (scaled by the eigenvalue 1), so we know there is no eigenvector basis, let alone the weaker result that there is no orthonormal eigenvector basis.

To apply the formal structure of QM we can prove this weaker result explicitly:
If $M$ above is diagonalizable, then $M = \ket{v_1}\bra{v_1} + \ket{v_2}\bra{v_2}$ which by the completeness relation 2.22 gives $M = I$, therefore $M$ is not diagonalizable.

\subsection{Exercise 2.13}

If $\ket{w}$ and $\ket{v}$ are any two vectors, show that
$(\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}$.

Since we are told that the adjoint is unique, it is sufficient to show equation 2.32:
\[(\ket{v'}, \ket{w}\braket{v}{w'}) =
(\ket{v}\braket{w}{v'}, \ket{w'})\]

Then by linearity of the inner product this is equivalent to
\[\braket{v}{w'}(\ket{v'}, \ket{w}) =
\braket{w}{v'}^*(\ket{v}, \ket{w'})\]
Apply conjugate symmetry and note that $\braket{v}{w'} = (\ket{v},
\ket{w'})$ by definition, and the result clearly follows.

\subsection{Exercise 2.14}

Show that the adjoint operation is anti-linear

Straight forward, again by uniqueness we simply need equation 2.32 to hold, which after linearity becomes:
\[\sum_i a_i(\ket{v}, A_i\ket{w}) = \sum_i
a_i^{**}(A_i^\dagger\ket{v}, \ket{w})\]
By double-conjugate elimination and the adjointness property, this is clearly true.

\subsection{Exercise 2.15}

Show that $(A^\dagger)^\dagger = A$.

\begin{align*}
	(\ket{v}, A^\dagger\ket{w})
	&= (A^\dagger\ket{w}, \ket{v})^* \\
	&= (\ket{w}, A\ket{v})^* \\
	&= (A\ket{v}, \ket{w})
\end{align*}
So $A$ is the adjoint of $A^\dagger$, i.e. by uniqueness of adjoints $(A^\dagger)^\dagger = A$.

\subsection{Equation 2.16}

Show that any projector $P$ satisfies the equation $P^2 = P$.

\begin{align*}
	(\sum_i \ket{i}\bra{i})^2
	&= \sum_{ij} \ket{i}\braket{i}{j}\bra{j}
	&= \sum_{ij} \delta_{ij}\ket{i}\bra{j}
	&= \sum_i \ket{i}\bra{i}
\end{align*}

\subsection{Exercise 2.17}
Show that a normal matrix is Hermitian if and only if it has real eigenvalues.

First show that a hermitian matrix has real eigenvalues, i.e. if
$A\ket{v} = v\ket{v}$ then $v$ is real.

Consider $\bra{v}A\ket{v}$:
\begin{align*}
	\bra{v}A\ket{v}
	&= (\ket{v}, A\ket{v}) \\
	&= (\ket{v}, v\ket{v}) \\
	&= v(\ket{v}, \ket{v}) \\
&= v\braket{v}{v}
\end{align*}
But by adjointness this can also be shown for $v^*$:
\begin{align*}
	\bra{v}A\ket{v}
	&= (\ket{v}, A\ket{v}) \\
	&= (A\ket{v}, \ket{v}) \\
	&= (v\ket{v}, \ket{v}) \\
	&= v^*(\ket{v}, \ket{v}) \\
&= v^*\braket{v}{v}
\end{align*}
So $v = v^*$, and necessarily $v$ is real.

Next show that if $A$ is normal and has all real eigenvalues, then $A$ is hermitian.

By spectral decomposition:
\[A = \sum_i v_i\ket{v_i}\bra{v_i}\]
Then taking the adjoint the result becomes obvious:
\begin{align*}
A^\dagger
	&= \sum_i v_i^* \ket{v_i}\bra{v_i} \\
	&= \sum_i v_i \ket{v_i}\bra{v_i} \\
&= A
\end{align*}

\subsection{Exercise 2.18}
Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i\theta}$ for some real $\theta$.

Suppose that $U\ket{v} = v\ket{v}$, then take the squared norm of both sides

\begin{align*}
&\bra{v}U^\dagger U\ket{v} = \bra{v} v^*v \ket{v} \\
\implies &\braket{v}{v} = \ord{v}^2\braket{v}{v} \\
\implies &\ord{v} = 1
\end{align*}

\subsection{Exercise 2.19}
Show that the Pauli matrices are Hermitian and unitary.

A matrix is hermitian \textit{and} unitary precisely when $U^\dagger = U$. $I$, $X$, and $Z$ are real symmetric matrices, so this is clearly true, and $Y$ has $i$ opposite $-i$ in its matrix representation, so it will satisfy this as well.

\subsection{Exercise 2.20}

Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$. Then the elements of $A'$ and $A''$ are $A'_{ij} = \bra{v_i}A\ket{v_j}$ and $A''_{ij} = \bra{w_i}A\ket{w_j}$. Characterize the relationship between $A'$ and $A''$.

Change of basis takes the form of a similarity/conjugation, and we have an intuition that orthonormal bases are related somehow to unitary operators, so a guess would be that $A'' = U^{-1}A'U$ for some unitary $U$. Take the $ij$th entry:

\[A'_{ij} = \sum_{lm} U^{-1}_{il}A''_{lm}U^{-1}_{mj}\]

At the same time we know that $A'$ and $A''$ represent the same operator, i.e.
\begin{equation}
\sum_{ij} A'_{ij}\ket{v_i}\bra{v_j} = \sum_{ij} A''_{ij}\ket{w_i}\bra{w_j}
\end{equation}

Combine these 

\[\sum_{ilmj} U^{-1}_{il}A''_{lm}U^{-1}_{mj}\ket{v_i}\bra{v_j} = \sum_{ij} A''_{ij}\ket{w_i}\bra{w_j}\]


Didn't work, apply both sides of (1) to $\ket{v_k}$

\[\sum_{ij} A'_{ij}\ket{v_i}\braket{v_j}{v_k} = \sum_{ij} A''_{ij}\ket{w_i}\braket{w_j}{v_k}\]
\[\sum_{i} A'_{ik}\ket{v_i} = \sum_{ij} A''_{ij}\ket{w_i}\braket{w_j}{v_k}\]

Didn't work, apply $\bra{v_l}\cdot\ket{m}$ to both sides of (1):
\[A'_{lm} = \sum_{ij} A''_{ij}\braket{v_l}{w_i}\braket{w_j}{v_m}\]

stumped... can't get any $\delta_{ij}$ magic to happen, the indices just keep growing!

\end{document}
