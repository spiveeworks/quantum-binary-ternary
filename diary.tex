\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\renewcommand\thesection{}
\renewcommand\thesubsection{}

% Title Page
\title{Honours Diary 2020}
\author{Jarvis Carroll}


\begin{document}
\maketitle

\section{Notation}

In this diary unless explicitly stated within a section, I have been using the notation specified by Nielsen and Chuang, with the following additions:
\begin{itemize}
	\item Implicit quantifiers for index variables such as $i, j, k$. (Nielson and Chuang seem to do this actually, perhaps dropping more than I do)
	\begin{itemize}
		\item $\{x_i\} = \{x_i\ |\ i \in I\}$, $\{|x_i\rangle\} = \{|x_i\rangle\ |\ i \in I\}$ etc.
		\item $(x_i) = (x_1, x_2, \ldots, x_n)$
		\item $\sum_i$ in place of $\sum_{i \in I}$
		\item $\forall i$ in place of $\forall i \in I$
	\end{itemize}
\end{itemize}

\section{March 13}

Set up TeXstudio and basic document structure.

\subsection{Exercise 2.1}
Linear Dependence, show that $(1,-1)$, $(1,2)$ and $(2,1)$ are linearly dependent.

\begin{align*}
	&(1, -1) + (1, 2) - (2, 1) \\
=\ &(1+1-2, -1+2-1) \\
=\ &(0, 0)
\end{align*}

\subsection{Exercise 2.2}
Matrix representations: Suppose $V$ is a vector space with basis vectors $|0\rangle$ and $|1\rangle$, and $A$ is a linear operator from $V$ to $V$ such that $A|0\rangle=|1\rangle$ and $A|1\rangle=|0\rangle$. Give a matrix representation for $A$, with respect to the input basis $|0\rangle$, $|1\rangle$, and the output basis $|0\rangle$, $|1\rangle$. Find input and output bases which give rise to a different matrix representation of $A$.

Equation 2.12 gives us the defining property of matrix representations:
\[A|v_j\rangle=\sum_i A_{ij}|w_i\rangle\]
This gives us a pair of vector equations:
\[
|1\rangle = A|0\rangle = A_{00}|0\rangle + A_{10}|1\rangle
\]
\[
|0\rangle = A|1\rangle = A_{01}|0\rangle + A_{11}|1\rangle
\]
By linear independence of $|0\rangle$, $|1\rangle$, it follows that
\begin{align*}
A_{00} &= 0 & A_{01} &= 1 \\
A_{10} &= 1 & A_{11} &= 0
\end{align*}
i.e. A has the matrix representation:
\[
A = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
\]

\section{March 16}

\subsection{Exercise 2.3}
\[
A: V \to W
\]\[
B: W \to X
\]\[
V = span\{|v_i\rangle\}\ \textit{etc.}
\]

At this point we are already identifying $A$, $B$ with their matrix representations. We would like to show equality between the function composition $B \circ A$ and the matrix product of the corresponding matrix representations, $B \times A$.
Once we have done this we will be able to identify both of these concepts as simply the expression $BA$, but for now we will use the explicit operators
% weird $\text{}$ layering to avoid soft-wrap putting \times on its own line
$\circ\ \text{and}\ \times$.

Our goal then is to show that the matrix representation of $B \circ A$ is $B \times A$.

\begin{align*}
&\sum_i (B \circ A)_{ij} |x_i\rangle &\text{(arbitrary column of matrix $B \circ A$)}\\
=\ &(B \circ A)|v_j\rangle &\text{(Definition of matrix representation)}\\
=\ &B(A|v_j\rangle) &\text{(Definition of composition)}\\
=\ &B\left(\sum_k A_{kj} |w_k\rangle\right) &\text{(Matrix representation)}\\
=\ &\sum_k A_{kj} (B |w_k\rangle) &\text{(Linearity of B)}\\
=\ &\sum_k A_{kj} \left( \sum_i B_{ik} |x_i\rangle \right) &\text{(Matrix representation)}\\
=\ &\sum_i \left(\sum_k B_{ik} A_{kj}\right) |x_i\rangle &\text{(distribution)}\\
=\ &\sum_i (B \times A)_{ij} |x_i\rangle &\text{(matrix product)}
\end{align*}
So by linear independence of $|x_i\rangle$ we know that $(B \circ A)_{ij} = (B \times A)_{ij}$ for arbitrary $i, j$, i.e. the matrix representation of $B \circ A$ is the matrix product $B \times A$.

\subsection{Exercise 2.4}

\[I: V \to V\]
\[I|x_i\rangle = |x_i\rangle\]

We would like to show that $I_{ij} = \delta_{ij}$.

\begin{align*}
&\sum_i I_{ij} |x_i\rangle \\
= &I|x_j\rangle \\
= &|x_j\rangle \\
= &\sum_i \delta_{ij}|x_i\rangle
\end{align*}

Again by linear independence of $\{|x_i\rangle\}$ we have $I_{ij} = \delta_{ij}$.

\subsection{Exercise 2.5}

\[((y_i), (z_i)) = \sum_i y_i^*z_i\]

We need to prove 3 properties. First linearity, taking $|v\rangle = (v_j) = (v_1, v_2, \ldots, v_n)$ and $|w_i\rangle = (w_{ij}) = (w_{i1}, w_{i2}, \ldots, w_{in})$.

For this we define
\[|z\rangle = (z_j) = \sum_i\lambda_i|w_i\rangle\]
Then by the definitions of sum and scalar product in $\mathbb{C}^n$ we observe
\begin{align*}
&z_j \\
=\ &\left(\sum_i\lambda_i|w_i\rangle\right)_j \\
=\ &\sum_i\left(\lambda_i|w_i\rangle\right)_j \\
=\ &\sum_i\lambda_iw_{ij}
\end{align*}
With this linearity falls out.
\begin{align*}
&\left(|v\rangle, |z\rangle\right) \\
=\ &\left((v_j), (z_j)\right) \\
=\ &\sum_j y_j^*z_j \\
=\ &\sum_j y_j^*\left(\sum_i\lambda_iw_{ij}\right) \\
=\ &\sum_i\lambda_i\left(\sum_jv_j^*w_{ij}\right) \\
=\ &\sum_i\lambda_i\left((v_j), (w_{ij})\right) \\
=\ &\sum_i\lambda_i\left(|v\rangle, |w_i\rangle\right)
\end{align*}

Next we prove conjugate symmetry.
\begin{align*}
&(|w\rangle, |v\rangle)^* \\
=\ &((w_i), (v_i))^* \\
=\ &\left(\sum_iw_i^*v_i\right)^* \\
=\ &\sum_iw_i^{**}v_i^* \\
=\ &\sum_iv_i^*w_i \\
=\ &((v_i), (w_i)) \\
=\ &(|v\rangle, |w\rangle)
\end{align*}

Finally positivity:
\begin{align*}
&(|v\rangle, |v\rangle) \\
=\ &((v_i), (v_i)) \\
=\ &\sum_iv_i^*v_i \\
=\ &\sum_i|v_i|^2
\end{align*}
Clearly this expression is at least 0, with equality when $v_i = 0\ \forall i$, i.e. when $|v\rangle = (0, 0, \ldots, 0)$.

Therefore the operator $(\cdot, \cdot)$ is an inner product on the vector space $\mathbb{C}^n$.

\section{March 23}

\subsection{Exercise 2.6}

Combines second argument linearity with conjugate symmetry, as you would expect.
\begin{align*}
\left(\sum_i\lambda_i|w_i\rangle, |v\rangle\right)
&= 
\left(|v\rangle, \sum_i\lambda_i|w_i\rangle\right)^* \\
&= \left(\sum_i\lambda_i\left(|v\rangle, |w_i\rangle\right)\right)^* \\
&= \sum_i\lambda_i^*\left(|v\rangle, |w_i\rangle\right)^* \\
&= \sum_i\lambda_i^*\left(|w_i\rangle, |v\rangle\right)
\end{align*}

\subsection{Exercise 2.7}

\[\langle w|v\rangle = (1)(1)+(1)(-1) = 0\]
\[|w\rangle, |v\rangle = \left(\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}}\right)\]

\subsection{Exercise 2.8}

Suppose that $j < k$, and that the first $k-1$ vectors are orthonormal,
\begin{align*}
\langle v_j| v_k\rangle
&= \langle v_j|w_k\rangle - \sum_i^{k-1}\langle v_i | w_k\rangle \langle v_j|v_i\rangle \\
&= \langle v_j|w_k\rangle - \langle v_j | w_k\rangle \langle v_j|v_j\rangle \\
&= \langle v_j|w_k\rangle - \langle v_j | w_k\rangle \langle v_j|v_j\rangle \\
&= 0
\end{align*}

Obviously $|v_i\rangle$ are explicitly normalized so we are done.

\section{March 27}

Feel like generalizing Ex 2.9 based on Ex 2.10.

\subsection{Exercise 2.10}

Suppose $|v_i\rangle$ is an orthonormal basis for an inner product space $V$.What is the matrix representation for the operator $|v_j\rangle\langle v_k|$, with respect to the $|v_i\rangle$ basis?

Let $A = |v_j\rangle\langle v_k|$, and $A|v_n\rangle = A_{mn}|v_m\rangle$, then
\begin{align*}
A_{mn}|v_m\rangle
&= |v_j\rangle \langle v_k|v_n\rangle \\
&= \delta_{kn}|v_j\rangle \\
\implies A_{mn}
&= \begin{cases}
\delta_{kn} & m = j \\
0 & else
\end{cases} \\
&= \begin{cases}
1 & m = j, n = k \\
0 & else
\end{cases}
\end{align*}

e.g. in a 2d space,
\[|0\rangle\langle 1| = \left[\begin{matrix}
0 & 1 \\
0 & 0
\end{matrix}\right]\]

\subsection{Exercise 2.9}

From 2.10 it becomes clear that
\[
I = \left[\begin{matrix}
1 & 0 \\
0 & 1
\end{matrix}\right] = |0\rangle\langle 0| + |1\rangle\langle 1|
\]
\[
X = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right] = |0\rangle\langle 1| + |1\rangle\langle 0|
\]
\[
Y = \left[\begin{matrix}
0 & -i \\
i & 0
\end{matrix}\right] = i|1\rangle\langle 0| -i|0\rangle\langle 1|
\]
\[
Z = \left[\begin{matrix}
1 & 0 \\
0 & -1
\end{matrix}\right] = |0\rangle\langle 0| - |1\rangle\langle 1|
\]

\subsection{Matrix Reps and Outer Products}

Visually one can see that we can use $|v_i\rangle \langle v_j|$ as a basis for describing linear maps directly in terms of their matrix representation, e.g.

\[\left[\begin{matrix}a&b\\c&d\end{matrix}\right] = a|0\rangle\langle 0| + b|0\rangle\langle 1| + c|1\rangle\langle 0| + d|1\rangle\langle 1|\]

Stated more generally

\[A = \sum_{ij} A_{ij}|v_i\rangle\langle v_j|\]

Which connects to equation 2.25 via the equality

\[\langle v_j| A | v_i\rangle = A_{ij}\]

Each of these things can be shown, especially once linear maps are themselves understood as a vector space, but I shall take these statements as given.

\subsection{Exercise 2.11}
Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.

Z is already diagonal.

X has the familiar and intuitive eigenvectors $|+\rangle$ and $|-\rangle$:
\begin{align*}
X|+\rangle = |+\rangle \\
X|-\rangle = -|-\rangle
\end{align*}
Diagonal representation then is $X = |+\rangle\langle+|\ -\ |-\rangle\langle-|$

$Y$ has the same shape as $X$ so we could guess the eigenvectors are the same, but $Y|+\rangle = -i|-\rangle$.

Solving properly we get $c(\lambda) = \lambda^2 - 1 = 0$, with eigenvectors in the null-spaces of:
\[
\left[\begin{matrix}-1&-i\\i&-1\end{matrix}\right]
\left[\begin{matrix}a\\b\end{matrix}\right]
=\left[\begin{matrix}0\\0\end{matrix}
\right]
\]
Second row is $-i$ times first row:
\[Y(|0\rangle -i |1\rangle) = -|0\rangle +i|1\rangle\]
Taking the conjugate we unsurprisingly get
\[Y(|0\rangle +i |1\rangle) = |0\rangle +i|1\rangle\]
Their diagonal representation would be just what you'd expect, $|v_0\rangle\langle v_0| - |v_1\rangle\langle v_1|$ where $|v_0\rangle = 2^{-\frac{1}{2}}\left(|0\rangle + i|1\rangle\right)$, $|v_1\rangle = 2^{-\frac{1}{2}}\left(|0\rangle - i|1\rangle\right)$.

\end{document}          
