\documentclass[]{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}

\renewcommand\thesection{}
\renewcommand\thesubsection{}

\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}

\newcommand{\ord}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}

\newcommand{\CNOT}{\text{CNOT}}

% Title Page
\title{Honours Diary 2020}
\author{Jarvis Carroll}


\begin{document}
\maketitle

\section{Notation}

In this diary unless explicitly stated within a section, I have been using the notation specified by Nielsen and Chuang, with the following additions:
\begin{itemize}
	\item Implicit quantifiers for index variables such as $i, j, k$. (Nielson and Chuang seem to do this actually, perhaps dropping more than I do)
	\begin{itemize}
		\item $\{x_i\} = \{x_i\ |\ i \in I\}$, $\{\ket{x_i}\} = \{\ket{x_i}\ |\ i \in I\}$ etc.
		\item $(x_i) = (x_1, x_2, \ldots, x_n)$
		\item $\sum_i$ in place of $\sum_{i \in I}$
		\item $\forall i$ in place of $\forall i \in I$
	\end{itemize}
\end{itemize}

\section{March 13}

Set up TeXstudio and basic document structure.

\subsection{Exercise 2.1}
Linear Dependence, show that $(1,-1)$, $(1,2)$ and $(2,1)$ are linearly dependent.

\begin{align*}
	&(1, -1) + (1, 2) - (2, 1) \\
=\ &(1+1-2, -1+2-1) \\
=\ &(0, 0)
\end{align*}

\subsection{Exercise 2.2}
Matrix representations: Suppose $V$ is a vector space with basis vectors
$\ket{0}$ and $\ket{1}$, and $A$ is a linear operator from $V$ to $V$ such that
$A\ket{0}=\ket{1}$ and $A\ket{1}=\ket{0}$. Give a matrix representation for
$A$, with respect to the input basis $\ket{0}$, $\ket{1}$, and the output basis
$\ket{0}$, $\ket{1}$. Find input and output bases which give rise to a different matrix representation of $A$.

Equation 2.12 gives us the defining property of matrix representations:
\[A\ket{v_j}=\sum_i A_{ij}\ket{w_i}\]
This gives us a pair of vector equations:
\[
	\ket{1} = A\ket{0} = A_{00}\ket{0} + A_{10}\ket{1}
\]
\[
	\ket{0} = A\ket{1} = A_{01}\ket{0} + A_{11}\ket{1}
\]
By linear independence of $\ket{0}$, $\ket{1}$, it follows that
\begin{align*}
A_{00} &= 0 & A_{01} &= 1 \\
A_{10} &= 1 & A_{11} &= 0
\end{align*}
i.e. A has the matrix representation:
\[
A = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
\]

\section{March 16}

\subsection{Exercise 2.3}
\[
A: V \to W
\]\[
B: W \to X
\]\[
	V = span\{\ket{v_i}\}\ \textit{etc.}
\]

At this point we are already identifying $A$, $B$ with their matrix representations. We would like to show equality between the function composition $B \circ A$ and the matrix product of the corresponding matrix representations, $B \times A$.
Once we have done this we will be able to identify both of these concepts as simply the expression $BA$, but for now we will use the explicit operators
% weird $\text{}$ layering to avoid soft-wrap putting \times on its own line
$\circ\ \text{and}\ \times$.

Our goal then is to show that the matrix representation of $B \circ A$ is $B \times A$.

\begin{align*}
	&\sum_i {(B \circ A)}_{ij} \ket{x_i} &\text{(arbitrary column of matrix $B \circ A$)}\\
	=\ &(B \circ A)\ket{v_j} &\text{(Definition of matrix representation)}\\
	=\ &B(A\ket{v_j}) &\text{(Definition of composition)}\\
	=\ &B\left(\sum_k A_{kj} \ket{w_k}\right) &\text{(Matrix representation)}\\
	=\ &\sum_k A_{kj} (B \ket{w_k}) &\text{(Linearity of B)}\\
	=\ &\sum_k A_{kj} \left( \sum_i B_{ik} \ket{x_i} \right) &\text{(Matrix representation)}\\
	=\ &\sum_i \left(\sum_k B_{ik} A_{kj}\right) \ket{x_i} &\text{(distribution)}\\
	=\ &\sum_i {(B \times A)}_{ij} \ket{x_i} &\text{(matrix product)}
\end{align*}
So by linear independence of $\ket{x_i}$ we know that ${(B \circ A)}_{ij} = {(B \times A)}_{ij}$ for arbitrary $i, j$, i.e.\ the matrix representation of $B \circ A$ is the matrix product $B \times A$.

\subsection{Exercise 2.4}

\[I: V \to V\]
\[I\ket{x_i} = \ket{x_i}\]

We would like to show that $I_{ij} = \delta_{ij}$.

\begin{align*}
	&\sum_i I_{ij} \ket{x_i} \\
	= &I\ket{x_j} \\
	= &\ket{x_j} \\
	= &\sum_i \delta_{ij}\ket{x_i}
\end{align*}

Again by linear independence of $\{\ket{x_i}\}$ we have $I_{ij} = \delta_{ij}$.

\subsection{Exercise 2.5}

\[((y_i), (z_i)) = \sum_i y_i^*z_i\]

We need to prove 3 properties. First linearity, taking $\ket{v} = (v_j) = (v_1,
v_2, \ldots, v_n)$ and $\ket{w_i} = (w_{ij}) = (w_{i1}, w_{i2}, \ldots, w_{in})$.

For this we define
\[\ket{z} = (z_j) = \sum_i\lambda_i\ket{w_i}\]
Then by the definitions of sum and scalar product in $\mathbb{C}^n$ we observe
\begin{align*}
&z_j \\
	=\ &{\left(\sum_i\lambda_i\ket{w_i}\right)}_j \\
	=\ &\sum_i{\left(\lambda_i\ket{w_i}\right)}_j \\
	=\ &\sum_i\lambda_i w_{ij}
\end{align*}
With this linearity falls out.
\begin{align*}
	&\left(\ket{v}, \ket{z}\right) \\
=\ &\left((v_j), (z_j)\right) \\
=\ &\sum_j y_j^*z_j \\
=\ &\sum_j y_j^*\left(\sum_i\lambda_i w_{ij}\right) \\
=\ &\sum_i\lambda_i\left(\sum_j v_j^*w_{ij}\right) \\
=\ &\sum_i\lambda_i\left((v_j), (w_{ij})\right) \\
	=\ &\sum_i\lambda_i\left(\ket{v}, \ket{w_i}\right)
\end{align*}

Next we prove conjugate symmetry.
\begin{align*}
	&{(\ket{w}, \ket{v})}^* \\
	=\ &{((w_i), (v_i))}^* \\
	=\ &{\left(\sum_i w_i^* v_i\right)}^* \\
	=\ &\sum_i w_i^{**}v_i^* \\
	=\ &\sum_i v_i^*w_i \\
	=\ &((v_i), (w_i)) \\
	=\ &(\ket{v}, \ket{w})
\end{align*}

Finally positivity:
\begin{align*}
	&(\ket{v}, \ket{v}) \\
=\ &((v_i), (v_i)) \\
=\ &\sum_i v_i^*v_i \\
=\ &\sum_i|v_i|^2
\end{align*}
Clearly this expression is at least 0, with equality when $v_i = 0\ \forall i$,
i.e.\ when $\ket{v} = (0, 0, \ldots, 0)$.

Therefore the operator $(\cdot, \cdot)$ is an inner product on the vector space $\mathbb{C}^n$.

\section{March 23}

\subsection{Exercise 2.6}

Combines second argument linearity with conjugate symmetry, as you would expect.
\begin{align*}
	\left(\sum_i\lambda_i\ket{w_i}, \ket{v}\right)
	&= {\left(\ket{v}, \sum_i\lambda_i\ket{w_i}\right)}^* \\
	&= {\left(\sum_i\lambda_i\left(\ket{v}, \ket{w_i}\right)\right)}^* \\
	&= \sum_i\lambda_i^*{\left(\ket{v}, \ket{w_i}\right)}^* \\
	&= \sum_i\lambda_i^*{\left(\ket{w_i}, \ket{v}\right)}
\end{align*}

\subsection{Exercise 2.7}

\[\braket{w}{v} = (1)(1)+(1)(-1) = 0\]
\[\ket{w}, \ket{v} = \left(\frac{1}{\sqrt{2}}, \pm\frac{1}{\sqrt{2}}\right)\]

\subsection{Exercise 2.8}

Suppose that $j < k$, and that the first $k-1$ vectors are orthonormal,
\begin{align*}
	\braket{v_j}{v_k}
	&= \braket{v_j}{w_k} - \sum_i^{k-1}\braket{v_i}{w_k}\braket{v_j}{v_i} \\
	&= \braket{v_j}{w_k} - \braket{v_j}{w_k} \braket{v_j}{v_j} \\
	&= \braket{v_j}{w_k} - \braket{v_j }{ w_k} \braket{v_j}{v_j} \\
&= 0
\end{align*}

Obviously $\ket{v_i}$ are explicitly normalized so we are done.

\section{March 27}

Feel like generalizing Ex 2.9 based on Ex 2.10.

\subsection{Exercise 2.10}

Suppose $\ket{v_i}$ is an orthonormal basis for an inner product space
$V$.What is the matrix representation for the operator
$\ket{v_j}\bra{v_k}$, with respect to the $\ket{v_i}$ basis?

Let $A = \ket{v_j}\bra{v_k}$, and $A\ket{v_n} = A_{mn}\ket{v_m}$, then
\begin{align*}
	A_{mn}\ket{v_m}
	&= \ket{v_j} \braket{v_k}{v_n} \\
	&= \delta_{kn}\ket{v_j} \\
\implies A_{mn}
&= \begin{cases}
\delta_{kn} & m = j \\
0 & else
\end{cases} \\
&= \begin{cases}
1 & m = j, n = k \\
0 & else
\end{cases}
\end{align*}

e.g. in a 2d space,
\[\ket{0}\bra{1} = \left[\begin{matrix}
0 & 1 \\
0 & 0
\end{matrix}\right]\]

\subsection{Exercise 2.9}

From 2.10 it becomes clear that
\[
I = \left[\begin{matrix}
1 & 0 \\
0 & 1
\end{matrix}\right] = \ket{0}\bra{0} + \ket{1}\bra{1}
\]
\[
X = \left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right] = \ket{0}\bra{1} + \ket{1}\bra{0}
\]
\[
Y = \left[\begin{matrix}
0 & -i \\
i & 0
\end{matrix}\right] = i\ket{1}\bra{0} -i\ket{0}\bra{1}
\]
\[
Z = \left[\begin{matrix}
1 & 0 \\
0 & -1
\end{matrix}\right] = \ket{0}\bra{0} - \ket{1}\bra{1}
\]

\subsection{Matrix Reps and Outer Products}

Visually one can see that we can use $\ket{v_i} \bra{v_j}$ as a basis for describing linear maps directly in terms of their matrix representation, e.g.

\[\left[\begin{matrix}a&b\\c&d\end{matrix}\right] = a\ket{0}\bra{0} +
	b\ket{0}\bra{1} + c\ket{1}\bra{0} + d\ket{1}\bra{1}\]

Stated more generally

\[A = \sum_{ij} A_{ij}\ket{v_i}\bra{v_j}\]

Which connects to equation 2.25 via the equality

\[\bra{v_j} A \ket{ v_i} = A_{ij}\]

Each of these things can be shown, especially once linear maps are themselves understood as a vector space, but I shall take these statements as given.

\subsection{Exercise 2.11}
Find the eigenvectors, eigenvalues, and diagonal representations of the Pauli matrices $X$, $Y$, and $Z$.

Z is already diagonal.

X has the familiar and intuitive eigenvectors $\ket{+}$ and $\ket{-}$:
\begin{align*}
	X\ket{+} = \ket{+} \\
	X\ket{-} = -\ket{-}
\end{align*}
Diagonal representation then is $X = \ket{+}\bra{+}\ -\ \ket{-}\bra{-}$

$Y$ has the same shape as $X$ so we could guess the eigenvectors are the same,
but $Y\ket{+} = -i\ket{-}$.

Solving properly we get $c(\lambda) = \lambda^2 - 1 = 0$, with eigenvectors in the null-spaces of:
\[
\left[\begin{matrix}-1&-i\\i&-1\end{matrix}\right]
\left[\begin{matrix}a\\b\end{matrix}\right]
=\left[\begin{matrix}0\\0\end{matrix}
\right]
\]
Second row is $-i$ times first row:
\[Y(\ket{0} -i \ket{1}) = -\ket{0} +i\ket{1}\]
Taking the conjugate we unsurprisingly get
\[Y(\ket{0} +i \ket{1}) = \ket{0} +i\ket{1}\]
Their diagonal representation would be just what you'd expect,
$\ket{v_0}\bra{v_0} - \ket{v_1}\bra{v_1}$ where $\ket{v_0} =
2^{-\frac{1}{2}}\left(\ket{0} + i\ket{1}\right)$, $\ket{v_1}
= 2^{-\frac{1}{2}}\left(\ket{0} - i\ket{1}\right)$.

\section{March 30}
\subsection{Exercise 2.12}
Prove that the matrix
\[\left[\begin{matrix}1&1\\1&0\end{matrix}\right]\]
is not diagonalizable.

The equation $c(\lambda) = (1-\lambda)^2 = 0$ has repeated root $\lambda = 1$, so it either has one eigenvector, or a eigenspace of 2 degenerate eigenvectors.

Clearly if it had 2 degenerate eigenvectors it would simply be the identity operation, mapping all vectors to themselves (scaled by the eigenvalue 1), so we know there is no eigenvector basis, let alone the weaker result that there is no orthonormal eigenvector basis.

To apply the formal structure of QM we can prove this weaker result explicitly:
If $M$ above is diagonalizable, then $M = \ket{v_1}\bra{v_1} + \ket{v_2}\bra{v_2}$ which by the completeness relation 2.22 gives $M = I$, therefore $M$ is not diagonalizable.

\subsection{Exercise 2.13}

If $\ket{w}$ and $\ket{v}$ are any two vectors, show that
$(\ket{w}\bra{v})^\dagger = \ket{v}\bra{w}$.

Since we are told that the adjoint is unique, it is sufficient to show equation 2.32:
\[(\ket{v'}, \ket{w}\braket{v}{w'}) =
(\ket{v}\braket{w}{v'}, \ket{w'})\]

Then by linearity of the inner product this is equivalent to
\[\braket{v}{w'}(\ket{v'}, \ket{w}) =
\braket{w}{v'}^*(\ket{v}, \ket{w'})\]
Apply conjugate symmetry and note that $\braket{v}{w'} = (\ket{v},
\ket{w'})$ by definition, and the result clearly follows.

\subsection{Exercise 2.14}

Show that the adjoint operation is anti-linear

Straight forward, again by uniqueness we simply need equation 2.32 to hold, which after linearity becomes:
\[\sum_i a_i(\ket{v}, A_i\ket{w}) = \sum_i
a_i^{**}(A_i^\dagger\ket{v}, \ket{w})\]
By double-conjugate elimination and the adjointness property, this is clearly true.

\subsection{Exercise 2.15}

Show that $(A^\dagger)^\dagger = A$.

\begin{align*}
	(\ket{v}, A^\dagger\ket{w})
	&= (A^\dagger\ket{w}, \ket{v})^* \\
	&= (\ket{w}, A\ket{v})^* \\
	&= (A\ket{v}, \ket{w})
\end{align*}
So $A$ is the adjoint of $A^\dagger$, i.e. by uniqueness of adjoints $(A^\dagger)^\dagger = A$.

\subsection{Equation 2.16}

Show that any projector $P$ satisfies the equation $P^2 = P$.

\begin{align*}
	(\sum_i \ket{i}\bra{i})^2
	&= \sum_{ij} \ket{i}\braket{i}{j}\bra{j}
	&= \sum_{ij} \delta_{ij}\ket{i}\bra{j}
	&= \sum_i \ket{i}\bra{i}
\end{align*}

\subsection{Exercise 2.17}
Show that a normal matrix is Hermitian if and only if it has real eigenvalues.

First show that a hermitian matrix has real eigenvalues, i.e. if
$A\ket{v} = v\ket{v}$ then $v$ is real.

Consider $\bra{v}A\ket{v}$:
\begin{align*}
	\bra{v}A\ket{v}
	&= (\ket{v}, A\ket{v}) \\
	&= (\ket{v}, v\ket{v}) \\
	&= v(\ket{v}, \ket{v}) \\
&= v\braket{v}{v}
\end{align*}
But by adjointness this can also be shown for $v^*$:
\begin{align*}
	\bra{v}A\ket{v}
	&= (\ket{v}, A\ket{v}) \\
	&= (A\ket{v}, \ket{v}) \\
	&= (v\ket{v}, \ket{v}) \\
	&= v^*(\ket{v}, \ket{v}) \\
&= v^*\braket{v}{v}
\end{align*}
So $v = v^*$, and necessarily $v$ is real.

Next show that if $A$ is normal and has all real eigenvalues, then $A$ is hermitian.

By spectral decomposition:
\[A = \sum_i v_i\ket{v_i}\bra{v_i}\]
Then taking the adjoint the result becomes obvious:
\begin{align*}
A^\dagger
	&= \sum_i v_i^* \ket{v_i}\bra{v_i} \\
	&= \sum_i v_i \ket{v_i}\bra{v_i} \\
&= A
\end{align*}

\section{April 3}

\subsection{Exercise 2.18}
Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i\theta}$ for some real $\theta$.

Suppose that $U\ket{v} = v\ket{v}$, then take the squared norm of both sides

\begin{align*}
&\bra{v}U^\dagger U\ket{v} = \bra{v} v^*v \ket{v} \\
\implies &\braket{v}{v} = \ord{v}^2\braket{v}{v} \\
\implies &\ord{v} = 1
\end{align*}

\subsection{Exercise 2.19}
Show that the Pauli matrices are Hermitian and unitary.

A matrix is hermitian \textit{and} unitary precisely when $U^\dagger = U$. $I$, $X$, and $Z$ are real symmetric matrices, so this is clearly true, and $Y$ has $i$ opposite $-i$ in its matrix representation, so it will satisfy this as well.

\subsection{Exercise 2.20}

Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$. Then the elements of $A'$ and $A''$ are $A'_{ij} = \bra{v_i}A\ket{v_j}$ and $A''_{ij} = \bra{w_i}A\ket{w_j}$. Characterize the relationship between $A'$ and $A''$.

Change of basis takes the form of a similarity/conjugation, and we have an intuition that orthonormal bases are related somehow to unitary operators, so a guess would be that $A'' = U^{-1}A'U$ for some unitary $U$. Take the $ij$th entry:

\[A'_{ij} = \sum_{lm} U^{-1}_{il}A''_{lm}U^{-1}_{mj}\]

At the same time we know that $A'$ and $A''$ represent the same operator, i.e.
\begin{equation}
\sum_{ij} A'_{ij}\ket{v_i}\bra{v_j} = \sum_{ij} A''_{ij}\ket{w_i}\bra{w_j}
\end{equation}

Combine these 

\[\sum_{ilmj} U^{-1}_{il}A''_{lm}U^{-1}_{mj}\ket{v_i}\bra{v_j} = \sum_{ij} A''_{ij}\ket{w_i}\bra{w_j}\]


Didn't work, apply both sides of (1) to $\ket{v_k}$

\[\sum_{ij} A'_{ij}\ket{v_i}\braket{v_j}{v_k} = \sum_{ij} A''_{ij}\ket{w_i}\braket{w_j}{v_k}\]
\[\sum_{i} A'_{ik}\ket{v_i} = \sum_{ij} A''_{ij}\ket{w_i}\braket{w_j}{v_k}\]

Didn't work, apply $\bra{v_l}\cdot\ket{m}$ to both sides of (1):
\[A'_{lm} = \sum_{ij} A''_{ij}\braket{v_l}{w_i}\braket{w_j}{v_m}\]

stumped... can't get any $\delta_{ij}$ magic to happen, the indices just keep growing!

\section{April 6}

\subsection{Exercise 2.21}
Repeat the proof of the spectral decomposition in Box 2.2 for the case when $M$ is Hermitian, simplifying the proof wherever possible

Let $P$ be a projector onto the $\lambda$ eigenspace of a hermitian matrix $M$.

Then $M=(P+Q)M(P+Q)=PMP+PMQ+QMP+QMQ$.

Note $MP = \lambda P$ and hence $PMP = \lambda P^2 = \lambda P$.

Then $QMP=(1-P)MP=MP-PMP=\lambda P - \lambda P = 0$.

Next $PMQ=(QM^\dagger P)^\dagger=(QMP)^\dagger=0^\dagger=0$.

Thus $M = PMP + QMQ$.

Next $QMQ$ is obviously hermitian since $(QMQ)^\dagger=QM^\dagger Q=QMQ$, so by induction is diagonalizable, so it follows that $M$ is diagonalizable.

\subsection{Exercise 2.22}
Prove that two eigenvectors of a Hermitian operator with different eigenvalues are necessarily orthogonal.

$M=M^\dagger$, $M\ket{v_1}=\lambda_1\ket{v_1}$, $M\ket{v_2}=\lambda_2\ket{v_2}$.

Observe the following equality:
\begin{align*}
\lambda_2\braket{v_1}{v_2}
&= \bra{v_1}\left(M\ket{v_2}\right) \\
&= \left(\bra{v_1}M^\dagger\right)\ket{v_2} \\
&= \lambda_1\braket{v_1}{v_2}
\end{align*}
\[\implies (\lambda_2-\lambda_1)\braket{v_1}{v_2} = 0\]
\[\implies \lambda_1 = \lambda_2 \text{\ or\ } \braket{v_1}{v_2} = 0\]

So if the eigenvalues of $\ket{v_1}$ and $\ket{v_2}$ are different, then $\ket{v_1}$ and $\ket{v_2}$ must be orthogonal.

\subsection{Exercise 2.23}

Show that the eigenvalues of a projector are all either 0 or 1.

Suppose $P\ket{w} = \lambda\ket{w}$, then by Ex. 2.16, $P^2=P$, so

\[\lambda\ket{w}=P\ket{w}=P^2\ket{w}=\lambda^2\ket{w}\]
\[\implies \lambda(1-\lambda)\ket{w}=0\]
So $\lambda$ is either 0 or 1.

\subsection{Exercise 2.24}
Show that a positive operator is necessarily Hermitian. (Hint: Show that an arbitrary operator $A$ can be written $A=B+iC$ where $B$ and $C$ are Hermitian.)

Similar to the matrix as a sum of symmetric and antisymmetric matrices, it's easy to see:
\begin{align*}
A &= \frac{A+A^\dagger}{2} + \frac{A-A^\dagger}{2} \\
&= \frac{A+A^\dagger}{2} + i\frac{iA^\dagger-iA}{2} \\
&= B + iC
\end{align*}

Then suppose $A$ is positive, i.e. $\bra{v}A\ket{v} \geq 0$.
\begin{align*}
\bra{v}A\ket{v}
&= \bra{v}(B + iC)\ket{v} \\
&= \bra{v}B\ket{v} + i\bra{v}C\ket{v}
\end{align*}

Note that since $B$ and $C$ are Hermitian, they are diagonalizable, with real eigenvalues, so $\bra{v}B\ket{v}$ must be real, and thus $\bra{v}C\ket{v} = 0$. Since $v$ is arbitrary, we have $C = 0$, so $A = B$ and therefore $A = A^\dagger$.

Note we don't require positivity, it is enough that $\bra{v}A\ket{v}$ is always real. This means that we have effectively generalized Ex 2.17 to the third equivalent condition that $\bra{v}A\ket{v}$ is always real.

\subsection{Exercise 2.25}

Show that for any operator $A$, $A^\dagger A$ is positive.

$\bra{v}A^\dagger A\ket{v} = (A\ket{v}, A\ket{v}) \geq 0$.

\section{April 13}

\subsection{Exercise 2.26}
Let $\ket{\psi}=(\ket{0}+\ket{1})/\sqrt{2}$. Write out $\ket{psi}^{\otimes 2}$ and $\ket{\psi}^{\otimes 3}$ explicitly, both in terms of tensor products like $\ket{0}\ket{1}$, and using the Kronecker product.

\begin{align*}
\ket{\psi}^{\otimes 2}
&= \frac{\ket{0} + \ket{1}}{\sqrt{2}} \otimes \frac{\ket{0} + \ket{1}}{\sqrt{2}}
\\&= \frac{1}{2}(\ket{0} + \ket{1})(\ket{0} + \ket{1})
\\&= \frac{1}{2}(\ket{00}+\ket{01}+\ket{10}+\ket{11})
\end{align*}

Similarly
\[\ket{\psi}^{\otimes 3} = \frac{1}{2\sqrt{2}}(\ket{000}+\ket{001}+\ket{010}+\ket{011} + \ket{100}+\ket{101}+\ket{110}+\ket{111})\]

Then in Kronecker product terms:

\[
\left(2^{-1/2}\left[\begin{matrix}1\\1\end{matrix}\right]\right)^{\otimes 2} = 2^{-1}\left[\begin{matrix}1\\1\\1\\1\end{matrix}\right]
\]

Skipping $\ket{\psi}^{\otimes 3}$

\subsection{Diversion: Non-normal Diagonalization}

This textbook deals with diagonalization of normal operators in the form $\sum_i \lambda_i\ket{v_i}\bra{v_i}$, but diagonalization of non-normal operators is also possible, and I would like to see how it looks in bra-ket notation.

\begin{align*}
A &= V\Lambda V^{-1}
\\&=
\left(\sum_i \ket{v_i}\bra{i}\right)
\left(\sum_i \lambda_i\ket{i}\bra{i}\right)
\left(\sum_i \ket{v_i}\bra{i}\right)^{-1}
\end{align*}

In the case that $V$ is unitary, i.e. $\ket{v_i}$ are orthogonal, then $V^{-1}=V^\dagger$ and we can simplify from there. Otherwise we have a general matrix inversion to do, which doesn't seem helpful at all.

\subsection{Diversion: Generalized Diagonalization}

Suppose instead that something only has generalized eigenvectors, but that these generalized eigenvectors are orthogonal. For example $A = \ket{0}\bra{1}$ with eigenvector $A\ket{0} = 0\ket{0}$ and generalized eigenvector $A\ket{1} = 1\ket{0}$.

This appears to motivate the form $A=\sum_i \lambda_i \ket{v_{\pi(i)}}\bra{v_i}$ so that $A\ket{v_i} = \lambda_i \ket{v_{\pi(i)}}$, but generalized eigenvectors seem more complicated than this, since the rule $(A + I)\ket{v_i} = (\lambda_i + 1)\ket{v_i}$ doesn't directly hold, and whatever its analogue might be, would be too much of a diversion for now.

\subsection{Exercise 2.27}
Calculate the matrix representation of the tensor products of the Pauli operators (a) $X$ and $Z$; (b) $I$ and $X$; (c) $X$ and $I$. Is the tensor product commutative?
\[
\left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
\otimes
\left[\begin{matrix}
1 & 0 \\
0 & -1
\end{matrix}\right]
=
\left[\begin{matrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1 \\
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0
\end{matrix}\right]
\]

\[
\left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
\otimes
\left[\begin{matrix}
1 & 0 \\
0 & 1
\end{matrix}\right]
=
\left[\begin{matrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{matrix}\right]
\]

\[
\left[\begin{matrix}
1 & 0 \\
0 & 1
\end{matrix}\right]
\otimes
\left[\begin{matrix}
0 & 1 \\
1 & 0
\end{matrix}\right]
=
\left[\begin{matrix}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{matrix}\right]
\]

\subsection{Exercise 2.28}

Show that the transpose, complex conjugation, and adjoint operations distribute over the tensor product,

\begin{align*}
&\left(\sum_{ij}A_{ij}\ket{j}\bra{i}\right)
\otimes
\left(\sum_{kl}B_{kl}\ket{l}\bra{k}\right)\ket{mn}
\\= &\sum_{jl}A_{mj}B_{nl} \ket{jl}
\end{align*}
So clearly by linearity
\begin{align*}
\left(\sum_{ij}A_{ij}\ket{j}\bra{i}\right)
\otimes
\left(\sum_{kl}B_{kl}\ket{l}\bra{k}\right)
=
\sum_{ijkl}A_{ij}B_{kl} \ket{jl}\bra{ik}
\end{align*}

From this all 3 results are obvious.

\section{April 20}

\subsection{Phase and Tensors}

Reading ahead to chapter 5, the Fourier transform, the algorithm for eigenvalue measurement first relies on a conditional $U^j$, which is used to modify the \textit{condition} rather than the vector to which it is applied.

Until now I have been ignoring phase, but this appears to be a direct application of it, so I have concocted a simpler example of phase shift through conditional operations on eigenvectors.

I believe that $\CNOT\left(\ket{+}\otimes\ket{-}\right) = \ket{-}\otimes\ket{-}$.

The matrix representation of CNOT is as follows:
\[
\CNOT = \left[\begin{matrix}
1&0&0&0\\
0&1&0&0\\
0&0&0&1\\
0&0&1&0
\end{matrix}\right]
\]
and $\ket{+}\otimes\ket{-}$ expanded in the computational basis is:
\begin{align*}
\ket{+}\otimes\ket{-}
&= \left(\ket{0}+\ket{1}\right) \otimes\left(\ket{0}-\ket{1}\right)
\\&= \ket{00}-\ket{01}+\ket{10}-\ket{11}
\end{align*}
and similarly $\ket{-}\otimes\ket{-}$ is:
\begin{align*}
\ket{-}\otimes\ket{-}
&= \left(\ket{0}-\ket{1}\right) \otimes\left(\ket{0}-\ket{1}\right)
\\&= \ket{00}-\ket{01}-\ket{10}+\ket{11}
\end{align*}
Clearly the signs of $\ket{10}$ and $\ket{01}$ have been permuted, so the equation makes sense.

I still find this consequence bizarre, and might have to wait until chapter 4 to better understand.

This example was constructed based on $\ket{-}$ being an eigenvector of $X$, so that performing $X$ on it would have the effect of performing $Z$ on the first bit instead! I guess really this is a curiosity of conditional operations and ultimately of the physical process that represents such an operation.

Or maybe this is best understood in terms of measurement? Although these two states are indistinguishable by measurement alone, so really it's $H$ that might present the phase as state and give us information. And since $H$ is an example of a Fourier transform, this is ultimately a curiosity of Fourier transform \textit{plus} conditional operations, so I haven't reduced the problem at all, just applied the simplest Fourier transform to the simplest conditional operation and gotten the same behavior.

This appears to be related to Exercise 4.34, and surrounding theory, which I shall get to eventually.

\section{April 27}

\subsection{Exercise 2.29, 2.30, 2.31, 2.32}
Show that the tensor product of two unitary/hermitian/positive/projector operators is unitary/hermitian/positive/a projector.

In the previous exercise we derived the following formula for the tensor product of two matrices:
\begin{align*}
\left(\sum_{ij}A_{ij}\ket{j}\bra{i}\right)
\otimes
\left(\sum_{kl}B_{kl}\ket{l}\bra{k}\right)
=
\sum_{ijkl}A_{ij}B_{kl} \ket{jl}\bra{ik}
\end{align*}
If $A$ and $B$ are normal, then we can use their eigenvectors as the basis, and this formula becomes the following:
\begin{align*}
\left(\sum_{i}v_i\ket{v_i}\bra{v_i}\right)
\otimes
\left(\sum_{j}u_j\ket{u_j}\bra{u_j}\right)
=
\sum_{ij}v_i u_j \left(\ket{v_i}\otimes\ket{u_j}\right) \left(\bra{v_i}\otimes\bra{u_j}\right)
\end{align*}

Clearly in this formula if $v_i$ and $u_j$ are all in some semigroup $S$, then their products will be as well, so the tensor of any operators with $S$ eigenvalues will be also have $S$ eigenvalues.

Setting $S =$ U(1) gives products of unitary operators are unitary.

Setting $S = \mathbb{R}$ gives products of Hermitian operators Hermitian.

Setting $S = [0, \infty)$ gives products of positive operators positive (and $(0, \infty)$ for positive definite)

Setting $S = \{0, 1\}$ gives products of projectors are projectors.
\subsection{Exercise 2.33}
The Hadamard operator on one qubit may be written as
\[H=\frac{1}{\sqrt{2}}\left[(\ket{0}+\ket{1})\bra{0}+(\ket{0}-\ket{1})\bra{1}\right]\]
Show explicitly that the Hadamard transform on $n$ qubits, $H^{\otimes n}$, may be written as 
\[H^{\otimes n}=\frac{1}{\sqrt{2^n}}\sum_{x,y}(-1)^{x \cdot y}\ket{x}\bra{y}\]

Clearly the formula holds for $n=1$, so we can take this as the base case of an induction.

Then if the formula holds for $n=k$, we can simply tensor this with the $n=1$ formula to get
\begin{align*}
H^{\otimes k+1}
&=
\left(\frac{1}{\sqrt{2^k}}\sum_{x=0}^{2^k-1}\sum_{y=0}^{2^k-1}(-1)^{x \cdot y}\ket{x}\bra{y}\right)
\otimes
\left(\frac{1}{\sqrt{2}}\sum_{x=0}^1\sum_{y=0}^1(-1)^{x \cdot y}\ket{x}\bra{y}\right)
\\&=
\frac{1}{\sqrt{2^{k+1}}}
\sum_{x_1=0}^{2^k-1}
\sum_{y_1=0}^{2^k-1}
\sum_{x_2=0}^1
\sum_{y_2=0}^1
(-1)^{x_1 \cdot y_1 + x_2 \cdot y_2}
\ket{x_1}
\ket{x_2}
\bra{y_1}
\bra{y_2}
\\&=
\frac{1}{\sqrt{2^{k+1}}}
\sum_{x_1=0}^{2^k-1}
\sum_{x_2=0}^1
\sum_{y_1=0}^{2^k-1}
\sum_{y_2=0}^1
(-1)^{(x_1 \otimes x_2) \cdot (y_1 \otimes y_2)}
\ket{x_1 \otimes x_2}
\bra{y_1 \otimes y_2}
\\&=
\frac{1}{\sqrt{2^{k+1}}}
\sum_{x=0}^{2^{k+1}-1}
\sum_{y=0}^{2^{k+1}-1}
(-1)^{x \cdot y}
\ket{x}
\bra{y}
\end{align*}
Where $x_1 \otimes x_2$ is just $2x_1 + x_2$

Write out an explicit matrix representation for $H^{\otimes 2}$.
\[
H^{\otimes 2} =
\left[\begin{matrix}
1 & 1 & 1 & 1\\
1 &-1 & 1 &-1\\
1 & 1 &-1 &-1\\
1 &-1 &-1 & 1
\end{matrix}\right]
\]

\section{May 1}
\subsection{Exercise 2.34}
Find the square root and logarithm of the matrix
\[A=\left[\begin{matrix}
4 & 3\\
3 & 4
\end{matrix}\right]\]
Note first the eigenvectors:
\[
\left[\begin{matrix}
4 & 3\\
3 & 4
\end{matrix}\right]
\left[\begin{matrix}
1\\
1
\end{matrix}\right]
=
\left[\begin{matrix}
7\\
7
\end{matrix}\right]
\]
\[
\left[\begin{matrix}
4 & 3\\
3 & 4
\end{matrix}\right]
\left[\begin{matrix}
1\\
-1
\end{matrix}\right]
=
\left[\begin{matrix}
1\\
-1
\end{matrix}\right]
\]
So we diagonalize as follows:
\begin{align*}
A = 7\ket{+}\bra{+}\ +\ \ket{-}\bra{-}
\\\sqrt{A} = \sqrt{7}\ket{+}\bra{+}\ +\ \ket{-}\bra{-}
\\=
\frac{1}{2}\left[\begin{matrix}
\sqrt{7}+1 & \sqrt{7}-1\\
\sqrt{7}-1 & \sqrt{7}+1
\end{matrix}\right]
\end{align*}

\subsection{Exercise 2.35}
\[\exp(i\theta v\cdot \sigma)=\cos(\theta)I+i\sin(\theta) v\cdot \sigma\]

First let's write out $v \cdot \sigma$ in matrix form.
\begin{align*}
v \cdot \sigma
&= v_1X + v_2Y + v_3Z
\\&=
\left[\begin{matrix}
v_3 & v_1 -iv_2\\
v_1+iv_2 & -v_3
\end{matrix}\right]
\end{align*}
Then the eigenvalue problem becomes
\begin{align*}
&v \cdot \sigma \ket{u} = \lambda \ket{u}
\\\implies& (v_3-\lambda)(-v_3-\lambda) -(v_1-iv_2)(v_1+iv_2) = 0
\\\implies& \lambda^2 - (v_1^2 + v_2^2 + v_3^2) = 0
\\\implies& \lambda = \pm \norm{v}
\end{align*}

And the eigenvectors will satisfy
\begin{align*}
\left[\begin{matrix}
v_3\mp\norm{v} & v_1-iv_2\\
v_1+iv_2 & -v_3\mp\norm{v}
\end{matrix}\right]
\left[\begin{matrix}
x\\
y
\end{matrix}\right]
=
\left[\begin{matrix}
0\\
0
\end{matrix}\right]
\\\implies
\left[\begin{matrix}
(v_1+iv_2)(v_3\mp\norm{v}) & v_1^2+v_2^2\\
(v_1+iv_2)(v_3\mp\norm{v}) & v_1^2+v_2^2
\end{matrix}\right]
\left[\begin{matrix}
x\\
y
\end{matrix}\right]
=
\left[\begin{matrix}
0\\
0
\end{matrix}\right]
\end{align*}
Clearly eigenvectors will be proportional to $(v_1^2+v_2^2, -(v_1+iv_2)(v_3\mp\norm{v}))$, but this isn't really important, we can observe the matrix is Hermitian and so will be diagonalizable with these real eigenvalues we found:

\begin{align*}
v\cdot \sigma
&= \norm{v}\ket{u_1}\bra{u_1} - \norm{v}\ket{u_2}\bra{u_2}
\\\implies \exp(i\theta v\cdot \sigma)
&= \exp(i\theta \norm{v}) \ket{u_1}\bra{u_1}
 + \exp(- i\theta \norm{v})\ket{u_2}\bra{u_2}
\\&= (\cos(\theta \norm{v})+i\sin(\theta\norm{v})) \ket{u_1}\bra{u_1}
 + (\cos(\theta \norm{v})-i\sin(\theta\norm{v}))\ket{u_2}\bra{u_2}
\\&= \cos(\theta \norm{v})(\ket{u_1}\bra{u_1}  + \ket{u_2}\bra{u_2})
 + i\sin(\theta\norm{v}) (\ket{u_1}\bra{u_1} - \ket{u_2}\bra{u_2})
\\&= \cos(\theta \norm{v})I
 + i\sin(\theta\norm{v}) \frac{1}{\norm{v}} v\cdot\sigma
\end{align*}
Then since $\norm{v}=1$ this gives the result.

\section{May 4}
It seems to be time to move on from chapter 2, even though I never got to the end of 2.1! The chapter will always be available as a reference, and I will see with time what parts deserve doing in full detail.

It currently feels as though my primary education is in "Quantum Algorithm Implementations for Beginners", so rather than read through the same summary of algorithms and operations that I have covered many times, and am currently covering again, I will try moving on to interesting questions of decomposition, and see both through these exercises and through reading what requires more detailed practice.

\subsection{Exercise 4.37}
Provide a decomposition of the level 4 Fourier transform:
\[
S_4 =
\frac{1}{2}
\left[\begin{matrix}
 1& 1& 1& 1\\
 1& i&-1&-i\\
 1&-1& 1&-1\\
 1&-i&-1& i
\end{matrix}\right]
\]

Define
\[\alpha = \frac{1}{\sqrt{2}} \implies 2\alpha^2=1\]
\[
U_{10} = \left[\begin{matrix}
\alpha&\alpha&0&0\\
\alpha&-\alpha&0&0\\
0&0&1&0\\
0&0&0&1
\end{matrix}\right]
\]
then
\[
U_{10}S_4 =
\frac{1}{2}
\left[\begin{matrix}
2\alpha&(1+i)\alpha&0&(1-i)\alpha\\
0&(1-i)\alpha&2\alpha&(1+i)\alpha\\
1&-1&1&-1\\
1&-i&-1&i
\end{matrix}\right]
\]
next repeat
\[\beta = \frac{1}{\sqrt{3}} \implies 3\beta^2=1\]
\[
U_{20} = \left[\begin{matrix}
2\alpha\beta&0&\beta&0\\
0&1&0&0\\
\beta&0&-2\alpha\beta&0\\
0&0&0&1
\end{matrix}\right]
\]
\[
U_{20}U_{10}S_4 =
\frac{1}{2}
\left[\begin{matrix}
3\beta&i\beta&\beta&-i\beta\\
0&(1-i)\alpha&2\alpha&(1+i)\alpha\\
0&(3+i)\alpha\beta&-2\alpha\beta&(3-i)\alpha\beta\\
1&-i&-1&i
\end{matrix}\right]
\]
and again
\[
U_{30} = \left[\begin{matrix}
3\alpha^2\beta&0&0&\alpha^2\\
0&1&0&0\\
0&0&1&0\\
\alpha^2&0&0&-3\alpha^2\beta
\end{matrix}\right]
\]
\[
U_{30}U_{20}U_{10}S_4 =
\frac{1}{2}
\left[\begin{matrix}
2&0&0&0\\
0&(1-i)\alpha&2\alpha&(1+i)\alpha\\
0&(3+i)\alpha\beta&-2\alpha\beta&(3-i)\alpha\beta\\
0&2i\beta&2\beta&-2i\beta
\end{matrix}\right]
\]

This looks like it might not be unitary, but I am out of time for today.
This is obviously taking a lot of time but it feels valuable to me.

\section{May 9}

Having written some C code to calculate Christoffel symbols yesterday, I'm thinking this might be a good place to write some C as well. The idea was to do it by hand to get a more intimate understanding of the process, but I think writing code will be equally intimate without the holdups from arithmetic error.

The code gave the same results as me, and seems to have reduced the fourier transform all the way down!

\url{https://github.com/spiveeworks/Archives/blob/c/unitary.c}

\section{May 11}

\subsection{Exercise 4.38}

Prove that there exists a $d\times d$ unitary matrix $U$ which cannot be decomposed as a product of fewer than $d-1$ two-level unitary matrices.

Not sure how to do this, but not really surprised by the result either.

I could imagine that the first $d-1$ reductions in the general process if put together would give a matrix with $d-1$ degrees of freedom, and so simply taking the quotient space $U(d)/U(2)^{d-1}$ one could observe a very large space of unreached matrices, but that still doesn't anywhere near prove the result.

Anyway I will move on since lower bounds aren't yet relevant.

\subsection{Exercise 4.39}

Find a quantum circuit using single qubit operations and CNOTs to implement the transformation
\[\left[\begin{matrix}
1&0&0&0&0&0&0&0\\
0&1&0&0&0&0&0&0\\
0&0&a&0&0&0&0&c\\
0&0&0&1&0&0&0&0\\
0&0&0&0&1&0&0&0\\
0&0&0&0&0&1&0&0\\
0&0&0&0&0&0&1&0\\
0&0&b&0&0&0&0&d
\end{matrix}\right]\]

The gray code will be that of $010 \mapsto 111$, which will be the same as the example given prior to this exercise without the second/second from last CNOTs.

\subsection{Discrete Basis}

Reading the explanations that follow on how discrete sets of gates are universal up to an arbitrary approximation, the crux of the argument is that $R = HTH$ is an irrational rotation around some plane, and hence as it is iterated will provide arbitrary rotations in this plane.

This is very far out of my domain of familiarity, so just reading through has taken some time, it seems to be a rephrasing of density of irrational spans $\langle 1, \theta \rangle$, which is not something that I realized would have practical applications, again because it is so far out of what I am familiar with.

I will leave this here I tried to do some thinking about how it works, I mean if $1$ and $\theta$ are linearly dependent then you can at least find a rational presentation of $\theta$, which I guess is part of the "pidgeonhole" argument used in the proof.

Anyway lots to learn I guess! Exercise 4.11 is referenced in the proof so I might go back to the start of chapter 4 soon, even though that won't help that much with density of irrationals.

\section{May 12}
Just a little recap on cubic roots of unity:

\begin{align*}
&(x+iy)^3 = x^3 + 3ix^2y - 3xy^2 -iy^3 = 1
\\\implies& 3x^2y - y^3 = 0
\\\implies& y = 0 \text{\ or\ } y = \pm x\sqrt{3}
\\\implies& x^3 - 3x(\pm x\sqrt{3})^2 = 1
\\\implies& x^3 = -\frac{1}{8}
\\\implies& x = -\frac{1}{2}
\end{align*}
So we have the solution:
\[\left(-\frac{1}{2} \pm \frac{\sqrt{3}}{2}i\right)^3 = 1\]

Unsurprisingly I am going to have to generalize my code to something that doesn't require both terms to have the same radical.

I might just use $\sqrt{n_1/d_1} + i\sqrt{n_2/d_2}$ and avoid the distinction between linear and quadratic representations. Addition needs to be checked and multiplication does not need to be checked, whatever I do.

Might do this tomorrow!

\section{May 16}

Returning to chapter 4 for some slower paced quantum gate practice, since I have the qutrit homework should I want to push myself.

\subsection{Exercise 4.1}
Map the eigenvectors of the Pauli matrices onto the Bloch sphere.

Exercise 2.11 gave us the eigenvectors:
\[
X = \ket{+}\bra{+} - \ket{-}\bra{-}
\]
\[Y = (S\ket{+})(\bra{+}S^\dagger) - (S\ket{-})(\bra{-}S^\dagger)\]
\[Z = \ket{0}\bra{0} - \ket{1}\bra{1}\]

These have the following Bloch coordinates:
\[
\ket{+} = \cos(\pi/4)\ket{0} + e^0\sin(\pi/4)\ket{1} \mapsto (1, 0, 0)
\]
\[
\ket{-} = \cos(-\pi/4)\ket{0} + e^0\sin(-\pi/4)\ket{1} \mapsto (-1, 0, 0)
\]
equivalently
\[
\ket{-} = \cos(\pi/4)\ket{0} + e^{i\pi}\sin(\pi/4)\ket{1} \mapsto (-1, 0, 0)
\]

\[S\ket{+} = \cos(\pi/4)\ket{0} + e^{i\pi/2}\sin(\pi/4)\ket{1} \mapsto (0, 1, 0)\]
\[S\ket{-} = \cos(-\pi/4)\ket{0} + e^{i\pi/2}\sin(-\pi/4)\ket{1} \mapsto (0, -1, 0)\]

\[\ket{0} = \cos(0)\ket{0} + e^0\sin(0)\ket{1} \mapsto (0, 0, 1)\]
\[\ket{1} = \cos(\pi/2)\ket{0} + e^0\sin(\pi/2)\ket{1} \mapsto (0, 0, -1)\]

\subsection{Eigenvectors of Unitary Hermitian Matrices on Bloch Sphere}

In Exercise 2.35 we are asked to exponentiate $i\theta(v_1X+v_2Y+v_3Z)$ and in doing so had to diagonalize an almost arbitrary unitary hermitian matrix $v_1X+v_2Y+v_3Z$.

It satisfied the following verbose eigenvector relation:
\[(v_1X+v_2Y+v_3Z)((v_1^2+v_2^2)\ket{0} - (v_1+iv_2)(v_3\mp \norm{v})\ket{1}) = \pm ((v_1^2+v_2^2)\ket{0} - (v_1+iv_2)(v_3\mp \norm{v})\ket{1})\]

The $\ket{0}$ coordinate looks suspiciously sphere-related, so it is tempting to see what $v = (\cos(\phi)\sin(\theta),\sin(\phi)\sin(\theta),\cos\theta)$ gives:
\[v_1^2+v_2^2=\sin^2(\theta)\ket{0}\]
\[-(v_1+iv_2)(v_3\mp\norm{v}) = -e^{i\phi}\sin(\theta)(\cos(\theta \mp 1))\]

So the $\phi$ almost maps directly, but the $\theta$ requires some rejigging? In any case it seems there is a very close relationship between the vector $v$ and the eigenvectors of $v \cdot \sigma$.

\subsection{Exercise 4.2}
Let $x$ be a real number and $A$ a matrix such that $A^2=I$. Show that 
\[\exp(iAx)=\cos(x)I+i\sin(x)A\]
Use this result to verify Equations (4.4) through (4.6).

Exercise 2.35 shows this equation holds for a related set of unitary hermitian matrices, from which (4.4) through (4.6) directly follow.

We will show this more general case now though.

If $A$ is normal then it will have eigenvalues satisfying $\lambda^2 = 1$, meaning it will be hermitian and unitary.

Then
\begin{align*}
\exp(iAx)
&= \exp\left(i\left(\sum_j v_j\ket{v_j}\bra{v_j}\right)x\right)
\\&= \sum_j \exp(iv_jx)\ket{v_j}\bra{v_i}
\\&= \sum_j (\cos(x)+iv_jsin(x))\ket{v_j}\bra{v_i}
\\&= \cos(x)\left(\sum_j \ket{v_j}\bra{v_i}\right)+isin(x)\left(\sum_j v_j\ket{v_j}\bra{v_i}\right)
\\&= \cos(x)I+isin(x)A
\end{align*}

If $A$ is not normal then its exponential is not a defined concept, but presumably $A^2=1$ implies normality? I'm not sure this has come up in the exercises.

\begin{align*}
\left[\begin{matrix}
a&b\\
c&d
\end{matrix}\right]^2
=
\left[\begin{matrix}
a^2+bc&b(a+d)\\
c(a+d)&bc+d^2
\end{matrix}\right]
=
\left[\begin{matrix}
1&0\\
0&1
\end{matrix}\right]
\end{align*}
\begin{align*}
\left[\begin{matrix}
a&b\\
c&d
\end{matrix}\right]
\left[\begin{matrix}
a&b\\
c&d
\end{matrix}\right]^\dagger
=
\left[\begin{matrix}
\ord{a}^2+\ord{b}^2&ac^*+bd^*\\
a^*c+b^*d&\ord{c}^2+\ord{d}^2
\end{matrix}\right]
\neq
\left[\begin{matrix}
a&b\\
c&d
\end{matrix}\right]^\dagger
\left[\begin{matrix}
a&b\\
c&d
\end{matrix}\right]
=
\left[\begin{matrix}
\ord{a}^2+\ord{c}^2&a^*b+c^*d\\
ab^*+cd^*&\ord{b}^2+\ord{d}^2
\end{matrix}\right]
\end{align*}

This gives the following possible relations:
\[a^2=d^2=1-bc\]
\[a=-d \text{\ or\ } b = c = 0\]
\[\ord{b} \neq \ord{c} \text{\ or\ } ac^*+bd^* \neq a^*b+c^*d\]

Now we definitely have $a=-d$, since otherwise we'd have $b=c=0$ which implies normality.

Then if $a=-d$ we could easily set $b=0$, $c=1$ to get non-normality, and then $a^2=1$ is sufficient for $A^2=I$.

\[
A=\left[\begin{matrix}
1&1\\
0&-1
\end{matrix}\right]\]
This is in fact not normal since it has the non-orthogonal eigenvectors $\ket{0}$ and $\ket{0}-2\ket{1}$.

This isn't relevant for quantum computation but in any case let's exponentiate it:
\begin{align*}
\exp(iAx)=
\exp(ix\left[\begin{matrix}
1&1\\
0&-2
\end{matrix}\right]
&\left[\begin{matrix}
1&0\\
0&-1
\end{matrix}\right]
\left[\begin{matrix}
1&1\\
0&-2
\end{matrix}\right]^{-1}
)
\\=
-\frac{1}{2}
\left[\begin{matrix}
1&1\\
0&-2
\end{matrix}\right]
&\left[\begin{matrix}
\exp(ix)&0\\
0&\exp(-ix)
\end{matrix}\right]
\left[\begin{matrix}
-2&-1\\
0&1
\end{matrix}\right]
\\=
-\frac{1}{2}
&\left[\begin{matrix}
\exp(ix)&\exp(-ix)\\
0&-2\exp(-ix)
\end{matrix}\right]
\left[\begin{matrix}
-2&-1\\
0&1
\end{matrix}\right]
\\=
&\left[\begin{matrix}
\exp(ix)&\sinh(ix)\\
0&\exp(-ix)
\end{matrix}\right]
\\=
\left[\begin{matrix}
\cos(x)&0\\
0&\cos(x)
\end{matrix}\right]
+i&\left[\begin{matrix}
\sin(x)&\sin(x)\\
0&-\sin(x)
\end{matrix}\right]
\\=\cos(x)I+&i\sin(x)A
\end{align*}

So interestingly the result applies for this matrix, I guess that you could prove it like follows:
\begin{align*}
&I=A^2=(V\Lambda V^{-1})^2=V\Lambda^2V^{-1}
\\\implies &\Lambda^2=I
\end{align*}
But $\Lambda$ is normal so it will be hermitian and unitary, then since conjugation is linear the result follows from the above proof for hermitian unitary matrices.
\begin{align*}
\exp(iAx)
&=\exp(iV\Lambda V^{-1}x)
\\&=V\exp(i\Lambda x)V^{-1}
\\&=V(\cos(x)I + i\sin(x)\Lambda)V^{-1}
\\&=\cos(x)VIV^{-1} + i\sin(x)V\Lambda V^{-1}
\\&=\cos(x)I + i\sin(x)A
\end{align*}

\end{document}
