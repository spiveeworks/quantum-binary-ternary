% chap1.tex (Chapter 1 of the thesis)

% Note that the text in the [] brackets is the one that will
% appear in the table of contents, whilst the text in the {}
% brackets will appear in the main thesis.

%[script style reduce fraction size]

%[quantikz]

%\section{Quantum Mechanics}

\chapter[PRELIMINARIES]{Preliminaries}
\section{Algebra of Unitary Matrices}
In quantum mechanics it will turn out crucial to have a strong theory of unitary operations acting linearly on complex-valued objects, so to that end we shall define these concepts and their notation here. [And sketch some theorems?]
\subsection{Hilbert Spaces}
Cartesian coordinates provide a powerful abstract way of reasoning about physical space as the combination of 3 variables, or conversely a way of visualizing combinations of variables as planes or volumes within a physical space. Hilbert spaces are a description which abstracts the Cartesian coordinate system even further, describing a much larger class of mathematical objects with similar geometric properties, including the space of possible states that a quantum object can take.

The abstract definition of a Hilbert space describes the following mathematical objects:
\begin{itemize}
	\item A set of `scalars', either $\mathbb{R}$ or $\mathbb{C}$
	\item A (nonempty) set of `vectors' $\mathbb{H}$
	\item A binary operation of vector addition: $\mathbb{H} \times \mathbb{H} \to \mathbb{H}$, written $u + v$ where $u$, $v$ are the vectors being added.
	\item An operation that `scales' vectors by a scalar: $\mathbb{K} \times \mathbb{H} \to \mathbb{H}$, simply written $av$ where $a$ is the scalar and $v$ is the vector
	\item An operation called the inner product, yielding a scalar for each pair of vectors: $\mathbb{H} \times \mathbb{H} \to \mathbb{K}$, written for now as $(u, v)$ where $u$, $v$ are the vectors involved.
	\item Well defined limits of any Cauchy sequence in $\mathbb{H}$, written $\lim_{n \to \infty} v_n$ where $(v_n)_{n=1}^\infty$ is the Cauchy sequence
\end{itemize}
Note that since $\mathbb{H}$ is non-empty, it must contain a `zero vector' acquired by scaling any element of $\mathbb{H}$ by 0. This vector is itself written as 0. When these objects are available, we say that $\mathbb{H}$ is a Hilbert space if it further satisfies the following algebraic properties, for any $a, b\in \mathbb{K}$, $u,v,w \in \mathbb{H}$:
\begin{itemize}
	\item vector associativity: $u + (v + w) = (u + v) + w$
	\item vector commutativity: $u + v = v + u$
	\item scalar identity: $1v = v$
	\item scalar associativity: $a(bv) = (ab)v$
	\item vector distribution: $a(u + v) = au + av$
	\item scalar distribution: $(a+b)v = av + bv$
	\item conjugate symmetry of inner products: $(u, v) = (v, u)^*$ (where $a^*$ is the complex conjugate of $a$)
	\item right linearity: $(u, v+w) = (u, v) + (u, w)$
	\item positive definite: $(v, v)$ real, and strictly positive whenever $v \neq 0$
\end{itemize}
Additionally Cauchy sequences must actually converge to their limits, a property whose formal definition and applications are beyond the scope of this thesis.

The primary Hilbert spaces discussed in this thesis are the sets of complex valued column vectors:
\[\mathbb{C}^n = \left\{\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]\ |\ a_0, a_1, \dots a_n \in \mathbb{C}\right\}\]
Adding and scaling of vectors of course take the usual definitions:
\[
c_0\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]
+
c_1\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
=
\left[\begin{matrix} c_0a_0+c_1b_0\\c_0a_1+c_1b_1\\\vdots\\c_0a_n+c_1b_n\end{matrix}\right]
\]
Inner products take a fairly natural definition:
\[
\left(
\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]
,
\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
\right)
=
\left[\begin{matrix} a_0^*&a_1^*&\cdots&a_n^*\end{matrix}\right]
\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
= \sum_{i=0}^{n-1} a_i^*b_i
\]
Inner products introduce a concept of orthogonality to a space of vectors, we say two vectors $u, v$ are orthogonal when $(u, v) = 0$.
\[
\left(\left[\begin{matrix}1\\0\end{matrix}\right],\left[\begin{matrix}0\\1\end{matrix}\right]\right) = 0
\]
In general if two column vectors $v_i$ and $v_j$ have coordinates 0 except for their $i$th and $j$th coordinate respectively, then $(v_i, v_j) = \delta_{ij}$, 1 when $i = j$ and 0 otherwise. This condition is what it means for the set $\{v_0 \dots v_{n-1}\}$ to be an orthonormal basis of the vector space $\mathbb{C}^n$. It is a fundamental result of Hilbert spaces that if a countable set of vectors $v_i \in \mathbb{H}$ span the whole space, that is if every vector $w \in \mathbb{H}$ is an infinite linear combination of the vectors $v_i$, then $\mathbb{H}$ has an orthonormal basis. If this orthonormal basis is finite then we say that $\mathbb{H}$ is finite dimensional, and can observe any vector $v \in \mathbb{H}$ as analogous to the following column vector in $\mathbb{C}^n$:
\[\left[\begin{matrix}
	(v_0, v)\\
	(v_1, v)\\
	\cdots\\
	(v_n, v)
\end{matrix}\right]\]
Notice that the column vectors corresponding to the basis vectors $v_i$ have coordinate 0 everywhere except for their $i$th co-ordinate which is 1. Further the co-ordinates of these column vectors are exactly the co-ordinates of $v$ in $\mathbb{H}$:
\[\left(v_i, \sum_{j=0}^{n-1} a_jv_j\right) = \sum_{j=0}^{n-1} a_j\left(v_i, v_j\right) = a_i\]
Finally the inner products of vectors $u, v \in \mathbb{H}$ are exactly the inner products of the corresponding column vectors:
\begin{align*}
	(u, v) &= \left(\sum_{i=0}a_iv_i, \sum_{j=0} b_jv_j\right)
	\\&= \sum_{j=0}b_j \left(\sum_{i=0}a_iv_i, v_j\right)
	\\&= \sum_{j=0}b_j \left(v_j, \sum_{i=0}a_iv_i\right)^*
	\\&= \sum_{i,j=0}a_i^*b_j \left(v_j, v_i\right)^*
	\\&= \sum_{i,j=0}a_i^*b_j \delta_{ij}
	\\&= \sum_{i=0}a_i^*b_i
	\\&= \sum_{i=0}(v_i, u)^*(v_i, v)
	\\&= \left(
	\left[\begin{matrix}
		(v_0, u)\\
		(v_1, u)\\
		\cdots\\
		(v_n, u)
	\end{matrix}\right]
	,
	\left[\begin{matrix}
		(v_0, v)\\
		(v_1, v)\\
		\cdots\\
		(v_n, v)
	\end{matrix}\right]
	\right)
\end{align*}
[could prove conjugate linearity earlier, or stop proving linear algebra results altogether!!]

In this sense any finite dimensional Hilbert space is geometrically equivalent to the complex vector space $\mathbb{C}^n$ where $n$ is the size of any orthonormal basis of $\mathbb{H}$.

As we shall describe in Chapter 1, quantum computation most commonly acts on Hilbert spaces that are finite dimensional, and while the specific space being considered can be important in practical contexts, this thesis is concerned with the theoretical and therefore only needs to understand the complex vector space $\mathbb{C}^n$.

One caveat to this equivalence is the concept of basis-independence, that many concepts in linear algebra are made useful by the fact that they give the same result regardless of which basis is used for the space in question. To show that something is basis independent, it is sufficient and often convenient to define it directly in terms of the inner product and vector operations, rather than the coordinate system induced by a given basis. As an example the $\ell^2$ norm in $\mathbb{C}^n$ does not depend on the basis used:
\[\norm{v}_2 = \sqrt{(v, v)} = \sqrt{\sum_{i=0}^{n-1}\ord{v_i}^2}\]
This cannot be done for any other $\ell^p$ norm.

\subsection{Dirac Notation}
There are a number of notations available for reasoning about linear algebra and vector spaces, but we shall see [do we?] that in order to reason about quantum algorithms we will rely heavily on the following techniques:
\begin{itemize}
	\item linear operators defined in terms of inner products
	\item change of basis via unitary operators
	\item change of index set when summing vectors
\end{itemize}
For this collection of techniques the Dirac notation for vectors and linear operators is particularly well suited.

The main feature of Dirac notation is the ket, where a bar and angle bracket are used to distinguish a symbol as representing a vector: $\ket{v}$, $\ket{*}$, $\ket{0}$, $\ket{i}$, etc.

The most common vectors used are the basis vectors, and in Dirac notation it is natural to use the index of each basis vector as the symbol \textit{for the vector itself}:
\begin{align*}
	\ket{0} = \left[\begin{matrix}
		1\\
		0
	\end{matrix}\right]
	&&&
	\ket{1} = \left[\begin{matrix}
		0\\
		1
	\end{matrix}\right]
\end{align*}

The inner product of two vectors $(\ket{u}, \ket{v})$ is normally written $\braket{u}{v}$, called a `bra-ket'. This is inspired by the notation $\langle u, v\rangle$, used before Dirac, but allows an elegant representation of co-vectors. Co-vectors map a vector to a scalar, and in an inner product space every vector has a natural co-vector defined using the inner product. In Dirac notation we can simply omit the ket from an inner product to notate a co-vector:
\begin{align*}
	\bra{u} &: \mathbb{C}^n \to \mathbb{C}
	\\\bra{u} &\equiv \ket{v} \mapsto \braket{u}{v}
\end{align*}

\subsection{Linear Operators}
If $U$ and $V$ are Hilbert spaces, (or generally vector spaces) then a map $A: U \to V$ is a linear operator if it satisfies the following in general:
\[A(a\ket{u} + b\ket{v}) = aA\ket{u} + bA\ket{v}\]
If we have vectors $\ket{u} \in U$ and $\ket{v} \in V$ then we can define a simple linear operator using the inner product in $U$:
\[\ket{v}\bra{u} \equiv \bra{u'} \mapsto \braket{u}{u'}\ket{v}\]
The fact that this is linear is a consequence of the inner product being linear in its second argument, one of the defining properties of Hilbert spaces.

Note that if we reverse the notation for scaling a vector we get the expression $\ket{v}\braket{u}{u'}$, which looks very similar to the application $\ket{v}\bra{u}(\ket{u'})$, again equivocating the map with the object it produces, just as was done in defining co-vectors.

As we have discussed, when dealing with any finite dimensional Hilbert space we can assume the space is equivalent to some complex vector space $\mathbb{C}^n$, in which case matrix representations of a linear operator become available. We derive matrix representation explicitly, since the relevant technology is readily available from Dirac notation. First observe that linear operators $A$ say, are determined uniquely by the image of each basis vector $\ket{j}$, say $A\ket{j} = \ket{v_j}$. First evaluate an arbitrary application of $A$:
\begin{align*}
	A\left(\sum_{j=0}^{n-1}u_j\ket{j}\right)
	&= \sum_{j=0}^{n-1}u_jA\ket{j}
	\\&= \sum_{j=0}^{n-1}u_j\ket{v_j}
\end{align*}
Then this turns out to be the same result that we get from applying the following:
\begin{align*}
	\left(\sum_{j=0}^{n-1}\ket{v_j}\bra{j}\right)\left(\sum_{i=0}^{n-1}u_i\ket{i}\right)
	&= \sum_{i=0}^{n-1}\sum_{j=0}^{n-1}u_i\braket{j}{i}\ket{v_j}
	\\&= \sum_{j=0}^{n-1}u_j\ket{v_j}
\end{align*}
Since these two maps are equal on their whole domain, they are the same:
\[A = \sum_{j=0}^{n-1} \ket{v_j}\bra{j}\]
Further if the codomain is also finite dimensional then we can repeat this process. First note that the outer product notation $\ket{v}\bra{u}$ is linear on $\ket{v}$, regardless of which vector $\ket{u'}$ it is applied to:
\[(a\ket{v_1}+b\ket{v_2})\braket{u}{u'} = a\ket{v_1}\braket{u}{u'}+b\ket{v_2}\braket{u}{u'}\]
Now suppose the image vectors $\ket{v_j}$ are in $\mathbb{C}^m$ and have coordinates themselves, $\ket{v_j} = \sum_i^{m-1} a_{ij}\ket{i}$, then:
\begin{align*}
	A = \sum_{i,j} a_{ij}\ket{i}\bra{j}
\end{align*}
That is, all linear operators are a linear combination of outer products between basis vectors, making these outer products a basis for the (Hilbert) space of linear operators between finite dimensional Hilbert spaces. We have derived a way of representing linear operators $\mathbb{C}^n \to \mathbb{C}^m$ as an array of $n \times m$ scalar coordinates, which is the exact structure we want from a matrix representation, and matrix multiplication appears directly from composing the corresponding maps:
\[\left(\sum_{i,j}a_{ij}\ket{i}\bra{j}\right)\left(\sum_{j',k}b_{j'k}\ket{j'}\bra{k}\right) = \sum_{i,k}\left(\sum_j a_{ij}b_{jk}\right)\ket{i}\bra{k}\]

For example any linear operator $A: \mathbb{C}^2 \to \mathbb{C}^2$ can be written as follows:
\begin{align*}
	A = \left[\begin{matrix}
		a&b\\
		c&d
	\end{matrix}\right]
	=&\ 
	a\left[\begin{matrix}
		1&0\\
		0&0
	\end{matrix}\right]
	+
	b\left[\begin{matrix}
		0&1\\
		0&0
	\end{matrix}\right]
	+
	c\left[\begin{matrix}
		0&0\\
		1&0
	\end{matrix}\right]
	+
	d\left[\begin{matrix}
		0&0\\
		0&1
	\end{matrix}\right]
	\\=&\  a\ket{0}\bra{0}+b\ket{0}\bra{1}+c\ket{1}\bra{0}+d\ket{1}\bra{1}
\end{align*}

When defining a linear operator $L: \mathbb{C}^n \to \mathbb{C}^n$ we can define it as the `linear extension' of a more succinct map $\phi: \{\ket{i}\ |\ 0 \leq i < n\} \to \mathbb{C}^n$, by first defining $\ket{v_j} = \phi(\ket{j})$ and then setting $L = \sum_{j} \ket{v_j}\bra{j}$. This is simply a matrix whose columns are the vectors $\ket{v_j}$.
\subsection{The Hermitian Adjoint and Diagonalizable Matrices}
An aspect of linear algebra essential to quantum computation is diagonalization. We shall describe a number of properties that a square matrix can have, in terms of both its Hermitian Adjoint, and its eigenvalues, but first we define these concepts.

The Hermitian Adjoint of a linear operator $A$ (square or otherwise) is the unique linear operator $A^\dagger$ that satisfies $\braket{u}{v'} = \braket{u'}{v}$ whenever $\ket{v'} = A\ket{v}$, and $\ket{u'} = A^\dagger \ket{u}$. It turns out that the matrix representation of $A^\dagger$ is exactly the complex conjugate of the transpose of $A$, so this is taken as the definition of the Hermitian Adjoint of a matrix. As an example observe the following matrix $A$ and its Hermitian adjoint:
\begin{align*}
	A = \left[\begin{matrix}
		1 & 1+i\\
		0 & 1
	\end{matrix}\right]
	&&&
	A^\dagger = \left[\begin{matrix}
		1 & 0\\
		1-i & 1
	\end{matrix}\right]
\end{align*}

Note now that in the vector space $\mathbb{C}^2$ the Hermitian adjoint of a vector has the same behaviour as a co-vector:
\begin{align*}
	\ket{u}^\dagger\ket{v}
	=&\ 
	\left[\begin{matrix}
		a\\
		b
	\end{matrix}\right]^\dagger
	\left[\begin{matrix}
		c\\
		d
	\end{matrix}\right]
	\\=&\ 
	\left[\begin{matrix}
		a^*&b^*
	\end{matrix}\right]
	\left[\begin{matrix}
		c\\
		d
	\end{matrix}\right]
	\\=&\ a^*c+b^*d
	\\=&\ \braket{u}{v}
\end{align*}
This generalizes to any co-vector in any Hilbert space.

Associativity of the matrix product also lets us represent the outer product $\ket{u}\bra{v}$ using the Hermitian adjoint, as the matrix product $\ket{u}\ket{v}^\dagger$. This can be used to prove that
$(\ket{u}\bra{v})^\dagger = \ket{v}\bra{u}$

Now we move on to the eigenvector problem, which is the problem of finding a scalar $\lambda$ and non-zero vector $\ket{v}$ so that $A\ket{v}=\lambda\ket{v}$. Such a $\lambda$ is called an eigenvalue of $A$, and such a $\ket{v}$ is called the corresponding eigenvector of $A$. For example if A has a non-trivial null-space, then $\lambda=0$ will be an eigenvalue of $A$, and any vector in the null-space of $A$ will be a corresponding eigenvector of $A$: $A\ket{v} = 0\ket{v}$.

If $\ket{v}$ is normalized, then the matrix $B=\lambda\ket{v}\bra{v}$ will also satisfy the eigenvalue problem $B\ket{v}=\lambda\ket{v}$, and any vector orthogonal to $\ket{v}$ will be in the null-space of $B$. This means that if all of the eigenvectors of $A$ are orthogonal to eachother, then we can write $A$ as a sum of such $B$ vectors. This structure $A = \sum_i \lambda_i \ket{v_i}\bra{v_i}$ tells us that $A$ acts like a diagonal matrix on its eigenvalues, which we call the diagonal representation of $A$, and for this reason call any such $A$ `diagonalizable'.

As an example, the following matrix $Z$ is already diagonal and so has a straight-forward diagonal representation:
\begin{align*}
	Z = \left[\begin{matrix}
		1&0\\
		0&-1
	\end{matrix}\right] = 1\ket{0}\bra{0} - 1\ket{1}\bra{1}
\end{align*}

Having orthogonal eigenvectors is closely related to the Hermitian adjoint through a class of matrices called `normal' matrices, where a matrix $A$ is normal if it satisfies $A^\dagger A = AA^\dagger$. A major result of linear algebra called the spectral theorem of normal matrices, is that a matrix is diagonalizable if and only if it is normal. What follows is a list of different classes of normal operator, each defined by a property of the matrix, and by an equivalent property of all of its eigenvalues:
\begin{align*}
	\text{Hermitian matrix:\ }&&& A^\dagger = A & \iff& \lambda \in \mathbb{R} \\
	\text{Unitary matrix:\ }&&& A^\dagger = A^{-1} & \iff& \ord{\lambda}=1 \\
	\text{Positive normal:\ }&&& \bra{v}A\ket{v} \in \mathbb{R}, \geq 0 & \iff& \lambda \in \mathbb{R}, \geq 0 \\
	\text{Positive definite normal:\ }&&& \bra{v}A\ket{v} \in \mathbb{R}, > 0 & \iff& \lambda \in \mathbb{R}, > 0 \\
	\text{Projection matrix:\ }&&& A^2 = A & \iff& \lambda \in \{0, 1\} \\
\end{align*}

The most relevant of these is the unitary matrix, whose defining property can also be written $AA^\dagger = I$, which is simply the matrix equation corresponding to the fact that the columns of $A$ form an orthonormal basis. In addition to the above, another characterisation of unitary matrices is that they preserve inner products, which follows naturally from the matrix interpretation of inner products:
\[(A\ket{u}, A\ket{v}) = (A\ket{u})^\dagger A\ket{v} = \bra{u}AA^\dagger \ket{v} = \braket{u}{v}\]
This means in particular that a unitary matrix will preserve $\ell^2$ norms, and will therefore induce an invertible operation on any (complex, centre at origin) $n$-sphere $\{\ket{v}\ |\ \braket{v}{v} = r^2\}$.

Finally, if $P$ is a polynomial then we can evaluate it on a square matrix $A$ as well, and it is easy to show that this polynomial `applies itself' to the eigenvalues of $A$, i.e.\ if $A\ket{v} = \lambda\ket{v}$ then $P(A)\ket{v} = P(\lambda)\ket{v}$.

Now any analytic function will be a limit of some polynomials, most notably the exponential:
\[e^x = \sum_n^\infty \frac{x^n}{n!}\]
This motivates us to apply such analytic functions directly to normal matrices, simply by applying it to each eigenvalue:
\begin{align*}
	f\left(\sum_i \lambda_i\ket{i}\bra{i}\right)
	&= \lim_{L \to \infty} \sum_{n=0}^L P_i\left(\sum_i \lambda_i\ket{i}\bra{i}\right)
	\\&= \lim_{L \to \infty} \sum_{n=0}^L \sum_i P_i(\lambda_i)\ket{i}\bra{i}
	\\&= \sum_i \lim_{L \to \infty} \left(\sum_{n=0}^L  P_i(\lambda_i)\right)\ket{i}\bra{i}
	\\&= \sum_i f(\lambda_i)\ket{i}\bra{i}
\end{align*}

This can be very useful for solving certain matrix equations such as $A^2 = B$ having the solution $A = \sqrt{B}$. It is also useful for mapping between Hermitian and unitary matrices with the map $U = e^{iH}$.

\subsection{Kronecker Products}
We have made explicit the way in which vectors, co-vectors, and linear operators are represented as matrices, that is as arrays of complex numbers. An operation that is useful for all of these objects is the tensor product, and while the tensor product can be described in the abstract as an operation between Hilbert spaces, the only cases of interest here are complex-valued matrices, whose tensor products are described by a much more concrete operation called the Kronecker product.

The Kronecker product has a fairly simple definition, if $A$ is an $m_1$ by $n_1$ matrix with elements $a_{ij}$, and $B$ is an $m_2$ by $n_2$ matrix with elements $b_{ij}$ then the Kronecker product $A \otimes B$ will be an $m_1m_2$ by $n_1n_2$ matrix and in block matrix form will look like the following:
\begin{align*}
	A &= \left[\begin{matrix}
		a_{00} & a_{01} & \dots & a_{0n_1}\\
		a_{10} & a_{11} & \dots & a_{1n_1}\\
		\vdots & \vdots & \ddots & \vdots\\
		a_{m_10} & a_{m_11} & \dots & a_{m_1n_1}
	\end{matrix}\right]
	\\\implies A\otimes B &= \left[\begin{matrix}
		a_{00}B & a_{01}B & \dots & a_{0n_1}B\\
		a_{10}B & a_{11}B & \dots & a_{1n_1}B\\
		\vdots & \vdots & \ddots & \vdots\\
		a_{m_10}B & a_{m_11}B & \dots & a_{m_1n_1}B
	\end{matrix}\right]
\end{align*}
Written more compactly, if $A\otimes B = C$ has elements $c_{m_2i_1 + i_2,n_2j_1+j_2}$, where $i_1$ is less than $m_1$, etc.\ then these elements of $C$ are exactly the products of elements of $A$ and $B$:
\[c_{m_2i_1 + i_2,n_2j_1+j_2} = a_{i_1j_1}b_{i_2j_2}\]

If $A$ is a vector $\ket{u}$ and $B$ is a co-vector $\bra{v}$ then their Kronecker product will be exactly the matrix product $AB$ which is exactly the outer product $\ket{u}\bra{v}$:
\[
A \otimes B =
\left[\begin{matrix}
	a_0\\a_1\\\vdots\\a_m
\end{matrix}\right]
\otimes
\left[\begin{matrix}
	b_0&b_1&\dots&b_n
\end{matrix}\right]
=
\left[\begin{matrix}
	a_0b_0 & a_0b_1 & \dots & a_0b_n\\
	a_1b_0 & a_1b_1 & \dots & a_1b_n\\
	\vdots & \vdots & \ddots & \vdots\\
	a_mb_0 & a_mb_1 & \dots & a_mb_n
\end{matrix}\right]
\]
On the other hand if $A$ and $B$ are both vectors, in $\mathbb{C}^{m_1}$ and $\mathbb{C}^{m_2}$ respectively, then their Kronecker product will be a vector in the larger space $\mathbb{C}^{m_1\times m_2}$, and in particular if $A$ and $B$ are canonical basis vectors $\ket{j_1}$ and $\ket{j_2}$ then their tensor product will be another canonical basis vector, $\ket{m_2j_1 + j_2}$. This allows the whole space $\mathbb{C}^{m_1 \times m_2}$ to be spanned by Kronecker products of $\mathbb{C}^{m_1}$ and $\mathbb{C}^{m_2}$.

The Kronecker product satisfies the following algebraic properties, all of which follow directly from their relevant definitions:
\begin{itemize}
	\item $(A\otimes B)(C \otimes D) = (AC) \otimes (BD)$
	\item in particular $(A\otimes B)(\ket{u} \otimes \ket{v}) = (A\ket{u})\otimes (B\ket{v})$
	\item $I_{n_1} \otimes I_{n_2} = I_{n_1\times n_2}$
	\item $(A\otimes B)^\dagger = A^\dagger \otimes B^\dagger$
	\item in particular $(\ket{u}\otimes \ket{v})^\dagger = \bra{u} \otimes \bra{v}$
	\item if $\lambda$ is a scalar then $\lambda (A \otimes B) = (\lambda A) \otimes B = A \otimes (\lambda B)$
	\item in particular $\lambda (\ket{u} \otimes \ket{v}) = (\lambda \ket{u}) \otimes \ket{v} = \ket{u} \otimes (\lambda \ket{v})$
\end{itemize}
With these properties we can show that the Kronecker product of two unitary matrices will be unitary:
\begin{align*}
	(A \otimes B)(A \otimes B)^\dagger
	&= (A \otimes B)(A^\dagger \otimes B^\dagger)
	\\&= AA^\dagger \otimes BB^\dagger
	\\&= I \otimes I
	\\&= I
\end{align*}

Considering the degrees of freedom involved, we expect that in general there are many $m_1m_2 \times n_1n_2$ matrices that are not the Kronecker product of an $m_1 \times n_1$ matrix with an $m_2 \times n_2$ matrix. Written in block matrix form it is easy to tell whether or not a matrix $C$ is a Kronecker product $A \otimes B$, by checking that every block $B_{ij}$ is a scalar multiple of any non-zero block $B$. The entries of $A$ are simply these scalar multiples, solving $B_{ij} = a_{ij}B$, and hence $C = A \otimes B$. (and if there are no non-zero blocks then $C = 0 = 0 \otimes 0$)
\subsection{The Unitary Group}
So far we have discussed a number of useful algebraic properties exhibited by unitary matrices, but the most basic properties of unitary matrices are of course their forming a group:
\begin{itemize}
	\item Product of unitaries is unitary: ${(AB)}^\dagger = B^\dagger A^\dagger = B^{-1}A^{-1} = {(AB)}^{-1}$
	\item Matrix products are always associative (since composition of any operator will be associative)
	\item The identity matrix is unitary: $I^\dagger = I = I^{-1}$
	\item $A^{-1} = A^\dagger$ always exists and is unitary
\end{itemize}

Quantum computation deals extensively with unitary operators, and so subgroups formed by particular sets of unitary operators will be a frequent point of discussion. A subgroup of a group is simply a subset that is still a group when equipped with the same group operation. Our group operation is matrix multiplication, and so the group of $n \times n$ unitaries written $U(n)$ is actually a subgroup of the much larger general linear group $GL(n, \mathbb{C})$, of $n \times n$ matrices with non-zero determinant.

A further subgroup of the set of unitary matrices $U(n)$ is the symmetric group $\mathcal{S}_n$, which we represent as the set of $n \times n$ permutation matrices. A permutation is an invertible function mapping a set to itself, and given a permutation on the canonical basis $\{\ket{i}\ |\ 0 \leq i < n\} \to \{\ket{i}\ |\ 0 \leq i < n\}$, we can extend this linearly to an invertible matrix $\mathbb{C}^n \to \mathbb{C}^n$ whose columns are all canonical basis vectors. We call such a matrix a permutation matrix, and note that the adjoint of its outer product form $P = \sum_i \ket{\sigma(i)}\bra{i}$ will be $P^{\dagger} = \sum_i \ket{i}\bra{\sigma{i}}$, giving the following:
\begin{align*}
	PP^\dagger = \sum_{i,j} \ket{\sigma(i)}\braket{i}{j}\bra{\sigma(j)} = \sum_i \ket{\sigma(i)}\bra{\sigma(i)} = I
\end{align*}

The symmetric group of any finite set is always finite, and is typically the first group considered when exploring finite subgroups of $U(n)$.
\subsection{Group Quotients, Normal Subgroups, Normalisers}
[perhaps an example of a group and its normalizer, group isomorphisms, and semidirect products. excessive definition stuff can go in axiomata appendix]

Given an equivalence relation $\sim$ on a set $S$ it is often useful to consider equivalence classes, the subsets $[x] = \{y\ |\ x \sim y\}$, since these will be equal exactly when their representatives are equivalent, i.e. $[x] = [y] \iff x \sim y$. The set of all such equivalence classes is called the set quotient, and is written $S/\sim$. It provides a concrete object with the same structural properties that would come from `identifying' $x$ with $y$ whenever $x \sim y$. If $S$ is actually a group $G$ then its group operation sometimes induces an operation in the set quotient $[x][y] = [xy]$, but if this is well defined then we immediately find $[e]$ is a subgroup of $S$:
\[e \sim x \sim y \implies [e] = [e][e] = [x][y] = [xy] \implies e \sim xy\]
\[e \sim x \implies [x^{-1}] = [e][x^{-1}] = [x][x^{-1}] = [xx^{-1}] = [e] \implies e \sim x^{-1}\]
Further if $x \in [e]$ and $z \in S$ then $zxz^{-1} \in [e]$ as well:
\[e \sim x \implies [z][x][z^{-1}] = [z][e][z^{-1}] = [e] \implies e \sim zxz^{-1}\]

In general if a subgroup $H$ of $G$ satisfies this condition, that for any $h \in H$ and $g \in G$, $ghg^{-1} \in H$, then $H$ is said to be a normal subgroup of $G$. It turns out that not only is the equivalence class $[e]$ a normal subgroup, but whenever $H$ is a normal subgroup of $G$, the equivalence relation $x \sim y \iff xy^{-1} \in H$ gives a well defined group operation on the set quotient $G/\sim$. We call this group the group quotient $G/H$. When available, this is a powerful tool for understanding the structure of groups, since the group quotient $G/H$ may have convenient algebraic properties emerging from its corresponding equivalence relation.

As an example of a normal subgroup, take $G \subset GL(n, \mathbb{R})$ to be any group formed by matrix multiplication, and $H$ to be the set of scalars in $G$, that is the set $\{\lambda I\ |\ \lambda \in \mathbb{C}\} \cap G$. Since scalars are commutative, it is straight-forward that $g\lambda I g^{-1} = \lambda gg^{-1} = \lambda I \in H$.

The subgroup relation and the normal subgroup relation can be thought of as a partial order, and are often written as $\leq$ and $\trianglelefteq$ respectively, since both are transitive, reflexive, and anti-symmetric. While these relations are each transitive, we must be careful when mixing them; if $H$ is a normal subgroup of $N$, and $N$ is a subgroup of $G$, $H$ is not necessarily a normal subgroup of $G$, which is surprising when stated more succinctly as ``$H$ is normal in $N$ but is not normal in $G$''.

With this subtlety in mind we can find exactly such a group $N$, given any subgroup $H$ of $G$. We call this the normaliser of $H$ with respect to $G$, defined as the set $N_G(H) = \{g\ |\ g \in G,\ gHg^{-1} \subseteq H\}$. This is the maximal such $N$, any other $N'$ with $H$ normal will sit inside $N_G(H)$. Clearly the group quotient $N_G(H)/H$ will exist for any subgroup $H$ of $G$, which will be useless in the case that $N_G(H) = H$, but otherwise can be an interesting group, and can even inform interesting structure about the normaliser itself, making normalisers a useful and novel tool for exploratory algebraic work.

\section{Quantum Mechanics}

The theory of Quantum mechanics is foundational in contemporary understanding of physical systems, and features phenomena radically different to that of the classical world when the objects under consideration are able to achieve `coherence', e.g.\ when sufficiently small or low in temperature.

Quantum mechanics is deliberately `incomplete' in the sense that it doesn't make specific predictions about the behaviour of atoms, electrons, or anything else, but rather poses constraints and structure that other physical theories such as quantum electrodynamics can adopt.

Quantum computation is the study of how quantum phenomena can be used to effect computation. When designing an individual experiment or programmable quantum computer it is necessary to use a concrete, empirically verified model of physical phenomena, but the specific algorithms that can be run on a quantum computer turn out to be described sufficiently by the general theory of quantum mechanics alone. Since this thesis is concerned with algorithms, we shall summarize the description of quantum mechanics given in the ubiquitous textbook "Quantum Computation and Quantum Information" by Nielsen and Chuang \cite{textbook}. This will provide a foundation for explaining certain techniques and algorithms prominent in quantum computing, and their relationship to the questions we aim to explore.
\subsection{State Space}
The first postulate of quantum mechanics is that any [closed?] quantum system can be modeled by some complex Hilbert space. While many of these Hilbert spaces are of infinite dimension, we can easily create finite dimensional Hilbert spaces by either trapping or ignoring the positions and other such properties of individual particles. Once we have an $n$-dimensional Hilbert space $\mathbb{H}$ modeling a system, we can substitute this for the equivalent Hilbert space $\mathbb{C}^n$, allowing us to model the system directly with this set.

We call the specific elements of $\mathbb{C}^n$ state vectors whenever they represent a state that a quantum computer can take, and we shall see soon that the state vectors are exactly the vectors of unit length in $\mathbb{C}^n$. This allows us to model quantum computation itself as an operation on these unit length state vectors, and to describe the initial, final, and intermediate states of any given computation as an individual state vector.

The simplest possible quantum system is an isolated quantum bit, or qubit, which can be modelled with the 2-dimensional Hilbert space $\mathbb{C}^2$:
\[
\mathbb{C}^2 = \left\{\left[\begin{matrix}
a\\
b
\end{matrix}\right]\ \middle|\ a, b \in \mathbb{C}\right\}
\]
\subsection{Evolution}
The second postulate of quantum mechanics can be stated in three equivalent ways: Given a closed quantum system with initial state vector $\ket{\phi}$ these 3 equivalent equations hold:
\[-i\hbar\frac{d}{dt}\ket{\phi} = H\ket{\phi}\]
\[\ket{\phi} = \exp\left(i \frac{tH}{\hbar}\right)\ket{\phi_0}\]
\[\ket{\phi} = U\ket{\phi_0}\]
Where $\hbar$ is the reduced Planck constant, which is a known scalar constant, $H$ is the Hamiltonian operator, a Hermitian operator associated with the energy present in the system, and $U$ is the unitary operator arising from fixing $t$ before exponentiating $itH/\hbar$.

A quantum computer, then, is simply a device that is able to manipulate the Hamiltonian $H$ of a quantum system, for specific durations of time $t$. When a quantum computer is capable of implementing a particular $H$, $t$ pair, we call the corresponding unitary matrix $U$ an elementary gate of the quantum computer. Composing these elementary gates will always give some overall unitary matrix $U = U_1U_2\dots U_k$, so the core of quantum computation then, is to understand how computationally interesting unitary matrices can be implemented using elementary gates of a quantum computer.
\subsection{Measurement}
Although the quantum systems we are discussing are represented as having a continuous state space of possible state vectors, a fundamental (and titular!) peculiarity of quantum mechanics is that when observed, a quantum object will always appear to be in states that are elements of some orthonormal basis of the corresponding Hilbert space. What is more peculiar, and more well known of quantum mechanics, is that the state that is measured becomes the new state of the system.

It is impossible to perform any computationally useful algorithm without measuring the result at some point in that algorithm, so in any theoretical discussion it is required that at least one procedure is possible for measuring the system. Whichever procedure is used, we call the corresponding orthonormal basis the `computational basis`. When identifying the given Hilbert space $\mathbb{H}$ with the vector space $\mathbb{C}^n$, we make sure to also identify the computational basis of $\mathbb{H}$ with the canonical basis $\{\ket{i}\}$ of $\mathbb{C}^n$, so that measurement can always be done in this basis.

Suppose we have a quantum system with state vector $\ket{\phi}$, and let $\ket{\phi}$ equal the following:
\[\ket{\phi} = \sum_{i=0}^n a_i\ket{i}\]

When measured this system will appear to be in state $\ket{i}$ with probability $\ord{a_i}^2$, and after measurement will be in the state $a_i'\ket{i}$, where $a_i'$ is the normalized complex number $\frac{a_i}{\ord{a_i}}$.

In more algebraic terms this can be stated as follows:
When measured this system will appear to be in state $\ket{i}$ with probability $\ord{\braket{i}{\phi}}^2$, and after measurement will be in the state that comes from applying the projection matrix $\ket{i}\bra{i}$ and normalizing.

Given that the coordinates of a state vector are now interpreted as being related to probabilities, it is important that state vectors have unit length:
\[\norm{\ket{\phi}} = \sum_{i=0}^n \ord{a_i}^2 = 1\]

A defining feature of unitary matrices is that they preserve vector magnitudes, meaning this assumption will be preserved at all points in time in a quantum algorithm.

As an example consider the $\ket{-}$ state:
\[\ket{-}=\frac{1}{\sqrt{2}}\ket{0} - \frac{1}{\sqrt{2}}\ket{1}\]
When measured, this will have probability $1/2$ of appearing to be in the $\ket{0}$ state, after which it will in fact be in the $\ket{0}$ state, and probability $1/2$ of appearing to be in the $\ket{1}$ state, although in actual fact it will be in the $-\ket{1}$ state.

In physical quantum experiments it is often possible to measure in multiple different bases, for example in the $\ket{+}$/$\ket{-}$ basis. While this may have practical advantages, the map between these bases will be unitary anyway so such non-computational measurements can be added or removed as needed by implementing the appropriate unitary instead, and by doing so we can get away with only using the computational basis, making for a more general theory.
\subsection{Composite State Space}
Suppose that we have two quantum systems, which when closed and isolated from each other would be modeled as $\mathbb{C}^m$ and $\mathbb{C}^n$ respectively. When we allow these to interact with each-other, and form a closed composite system, we will need some distinct Hilbert space with which to model them. Intuitively if we measure both systems we should find the first in some computational basis state $\ket{i}$ and the second in some second computational basis state $\ket{j}$, giving a total of $m \times n$ different measurements. The measurement postulate described for individual systems will apply just as well to this coupled system, and so we have to extend these $m \times n$ different pairs of states to the Hilbert space $\mathbb{C}^{m \times n}$, whose computational basis is now given by Kronecker products $\ket{i} \otimes \ket{j}$.

In general if we had of modeled the systems in states $\ket{v_1}$ and $\ket{v_2}$ then we can embed these states in the coupled system as the state $\ket{v_1} \otimes \ket{v_2}$. We shall see in the section on composite measurement that these Kronecker products are postulated to behave in the same way as the individual states when measured. As an example of these embedded states, consider the following pair of qubit states:
\begin{align*}
\ket{1} = \left[\begin{matrix}0\\1\end{matrix}\right]
&&&
\ket{+} = \left[\begin{matrix}\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
\end{align*}
When taking two such qubit objects as a coupled system, the corresponding Kronecker state would look like this:
\[
\ket{1}\otimes \ket{+} = \left[\begin{matrix}0\\1\end{matrix}\right]
\otimes
\left[\begin{matrix}\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
=
\left[\begin{matrix}0\\0\\\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
\]

Note that when taking the Kronecker product of multiple vectors or multiple co-vectors, we can suppress the $\otimes$ operator, simply writing it as a concatenated sequence of kets: $\ket{v_1}\ket{v_2} = \ket{v_1}\otimes\ket{v_2}$. This does not introduce any ambiguity since the matrix product of two vectors is not defined.

In classical computation we represent integers and other data as a sequence of small (typically binary) digits and in the same way we can represent integers as computational basis vectors $\ket{i_1}\ket{i_2}\dots\ket{i_n}$ in some large system made from composing individual `quantum digits', binary or otherwise. In other contexts it is common to take the corresponding sequence of digits as the identifier for a single ket $\ket{i_1i_2\dots i_n}$ but we shall refrain from doing this here, since it poses no significant advantage when dealing with the most foundational aspects of quantum computation.

As was described in our original discussion of Kronecker products, not all quantum states in a composite system $\mathbb{C}^m\otimes\mathbb{C}^n$ are of the form $\ket{u}\otimes\ket{v}$. States that can't be represented as a single Kronecker product $\ket{u}\otimes\ket{v}$ are called entangled states. Entanglement is a very important feature of quantum mechanics and quantum computation that will be discussed on its own later in this document. For now though observe the Bell state:

\[\frac{1}{\sqrt{2}}\ket{0}\otimes\ket{0} + \frac{1}{\sqrt{2}}\ket{1}\otimes\ket{1} = \left[\begin{matrix}
\frac{1}{\sqrt{2}}\\
0\\
0\\
\frac{1}{\sqrt{2}}
\end{matrix}\right]\]
We can see this has two non-zero blocks, $1/\sqrt{2}\ket{0}$ and $1/\sqrt{2}\ket{1}$ respectively, which are not multiples of each-other.

[entanglement as basis independent property?]

Seeing as classical systems aren't treated as having any form of superposition, there is no classical analogy for entangled states. It is worth noting that in classical systems a composite system would be understood to have a state space that is the Cartesian product of two individual state spaces, and that the Cartesian product of an $m$-dimensional vector space with an $n$-dimensional vector space (equipped with point-wise vector addition say, giving the direct sum of two vector spaces) would have dimension $m+n$ rather than the $m\times n$ dimensional space given by composite quantum systems.
\subsection{Composite Evolution}
Just as with individual systems, an isolated composite system evolves according to some unitary operator after any discrete time step. In the composite system $\mathbb{C}^{mn}$ this could be any $mn$ by $mn$ unitary matrix, and once again we call any such operation an elementary gate of a quantum computer whenever it can be reliably produced by that computer. Typically the elementary gates that can be implemented for individual systems will be implemented in the same way in composite systems, again using the Kronecker product to embed the corresponding operation into the composite model:
\[U \otimes I = \sum_i \ket{u_i}\otimes\ket{v_i} \mapsto \sum_i (U\ket{u_i})\otimes \ket{v_i}\]

For example the operation $I \otimes H$ acting on the 2-qubit system $\mathbb{C}^{2\times 2}$, which applies $H$ to the second qubit and leaves the first qubit unchanged, will have the following matrix representation:
\[
\left[\begin{matrix}
	1&0\\
	0&1
\end{matrix}\right]
\otimes
\left[\begin{matrix}
	\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
	\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}
\end{matrix}\right]
=
\left[\begin{matrix}
	\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0&0\\
	\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}&0&0\\
	0&0&\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
	0&0&\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}
\end{matrix}\right]
\]

If all of the elementary gates available to a quantum computer can be represented as the Kronecker product of individual gates, and the initial state of the computer can be represented as the Kronecker product of individual states, then the computer will never be able to create entangled states, and will essentially be two distinct quantum computers running in parallel. This means that it is crucial for a quantum computer to have at least one operation that is not the Kronecker product of individual operations, and the most common of these elementary gates is the controlled increment gate, which acts as a permutation on the computational basis $\ket{i}\ket{j} \mapsto \ket{i}\ket{i+j \mod n}$.

The controlled increment operation in binary systems is called the controlled-not operation, which will act mod 2, and therefore simply swap the $\ket{1}\ket{0}$ state with the $\ket{1}\ket{1}$ state. In this case we would call the first qubit the `control' object, and the second qubit the `target' object. This can not be represented as a Kronecker product of two individual operations, which can be seen in the following matrix representation:
\[
\left[\begin{matrix}
1&0&0&0\\
0&1&0&0\\
0&0&0&1\\
0&0&1&0
\end{matrix}\right]
\]
(In fact no controller increment operation will be a Kronecker product of individual operations, since their matrix representation will always have an $I$ block, and an $X$ block, which are not proportional.)

This notion of controlled operations is useful to generalize to any unitary operation acting on a smaller quantum system, which will be discussed in more detail in the context of both binary and ternary quantum systems later.

It is useful to note that the Kronecker product commutes with the outer product:
\[(\ket{u'}\bra{u})\otimes(\ket{v'}\bra{v}) = \ket{u'}\ket{v'}\bra{u}\bra{v}\]
One consequence of this is that as with state vectors, the Kronecker product of canonical basis operators $\ket{i'}\bra{i}$ and $\ket{j'}\bra{j}$ will give all of the canonical basis operators $\ket{i'}\ket{j'}\bra{i}\bra{j}$ on $\mathbb{C}^{m \times n}$.
\subsection{Composite Measurement}
When measuring an individual system we gave the algebraic definition that state $\ket{\phi}$ would be measured in state $\ket{i}$ with probability $\ord{\braket{i}{\phi}}^2$, and would then be in the state that comes from applying the projection $\ket{i}\bra{i}$ and normalizing.

This postulate stated as is would allow us to measure all of the individual components of the composite system at the same time, but it is actually much more powerful to measure one component by itself without otherwise disturbing the rest of the system. To that end suppose we have a composite quantum system whose state vector is the following:

\[\ket{\phi} = \sum_{0 \leq i < m}\sum_{0 \leq j < n} a_{ij}\ket{i}\ket{j}\]

Either the $\mathbb{C}^m$ or the $\mathbb{C}^n$ component of this system could be measured. Without loss of generality we shall describe measurement of the first state. The full statement of the measurement postulate [need to go through and make the postulates more explicit in the structure of this discussion] is that this system will be in state $\ket{i}$ with probability:
\[P(\ket{i}) = \sum_{j=0}^n\ord{a_{ij}}^2\]
After measuring the system will be in the state that comes from applying the projection operator $(\ket{i}\bra{i})\otimes I$ and then normalizing.

For example observe the following 2-qubit state vector $\ket{+}\ket{+}$:
\[\ket{+}\ket{+}=\frac{1}{2}\left(\ket{00}+\ket{01}+\ket{10}+\ket{11}\right)=\frac{1}{2}\left[\begin{matrix}
1\\
1\\
1\\
1
\end{matrix}\right]\]

If we measured the first qubit then we would observe $\ket{0}$ with probability $1/4+1/4 = 1/2$, and similarly $\ket{1}$ with probability $1/2$. Upon measuring $\ket{0}$ the overall state would be $\ket{0}\ket{+}$, whereas upon measuring $\ket{1}$ the overall state would be $\ket{1}\ket{+}$.

While this is straight-forward for states that are simply the Kronecker product of two states, measuring entangled states can be quite interesting. For example suppose we measured the first component of the Bell state $1/\sqrt{2}(\ket{00}+\ket{11})$. As before we would find it in state $\ket{0}$ with probability $1/2$, and $\ket{1}$ with probability $1/2$, but after measurement it would be in states $\ket{00}$ and $\ket{11}$ respectively. Measuring the first component of the system changed the second one! This is the basis of a number of novel features of quantum computing, quantum information theory, and quantum cryptography. [chuck in quantum teleportation]

[note somewhere that more than two systems continue naturally from these subsections]

\section{Quantum Computation}

With quantum mechanics and the associated concept of a quantum algorithm in place, we can begin to reason about the techniques of quantum computation. To this end we shall briefly discuss the computer science of probabilistic vs deterministic algorithms, as well as the group theory of different sets of unitary matrices, important concepts that lay the foundation of technical discussions in the field of quantum computation. With this in place we shall be prepared to discuss the state of the field when it comes to quantum computation on objects with more than two basis states.

\subsection{Probabilistic Algorithms}
In computer science there is a distinction between deterministic algorithms, and randomized/probabilistic/non-deterministic algorithms. In summary a deterministic algorithm is a sequence of exact steps that can be executed in order to compute a result, whereas a probabilistic algorithm is permitted to rely on some source of random information to determine its control flow, meaning that the same input could result in many different outputs.

The advantage of probabilistic algorithms is that they can often avoid the worst-case performance associated with certain input states in deterministic algorithms; for example, many implementations of sorting algorithms will take much longer than usual to sort a list in ascending order if it is initially in descending order. At an intuitive level such worst-case input states tend to exploit the specific order in which an algorithm explores its possible solutions, and so since a randomized algorithm has no single order in which it might explore solutions, such worst-case inputs do not exist.

On classical computers deterministic algorithms are often more natural, and even when a randomized algorithm is used it will likely use a pseudo-random number generator in place of a true random source of information. This is heavily contrasted with quantum computation and quantum physics more generally, where measuring the state of a quantum object is inherently probabilistic, and physical implementations of quantum algorithms often introduce physical sources of error as well, as a result the probabilistic quantum algorithm is taken as the norm, with the exception of algorithms that only measure computational basis states, which when simulated theoretically will act deterministically. In physical quantum computers to date noise has been of primary concern in the real world execution of algorithms, and so even these algorithms that are deterministic when simulated end up being non-deterministic in practice.

One important consequence of this is that a quantum algorithm that appears to perform well might still perform better on a classical computer with a source of randomness; when comparing quantum with classical, we must consider the probabilistic algorithms of both.

A further distinction in computer science is made between Las Vegas and Monte Carlo algorithms, the former being algorithms that always yield a result, but have random run-time, and the latter being algorithms that yield some result in fixed run-time, but have a random chance of failing or producing a result that is incorrect. Often an algorithm in one of these classes can be converted into the other, Las Vegas algorithms that repeatedly search for a solution can be modified to yield a false negative after a fixed number of attempts, and Monte Carlo algorithms that may produce a false negative can check their solution using a deterministic algorithm, and retry until a valid solution is found. This means that in classical contexts this distinction is less important than that of deterministic vs probabilistic algorithms, since both classes can achieve similar things with the same resources.

In quantum contexts however, algorithms are generally assumed to be Monte Carlo algorithms, since the sources of error discussed are sources of incorrect output, rather than sources of increased run-time. Further when a physical quantum computer is run, it is run repeatedly, often thousands of times, in order to determine the probability of each output, so that one can infer which output is the correct one.

When constructing unitary matrices out of some elementary gate set, it becomes possible to use approximate constructions, since all quantum algorithms are Monte Carlo by default, and small changes in a state's coordinates tend not to significantly affect the overall probability distribution of the algorithm's outputs. Often quantum algorithms and their associated unitary matrices are implemented asymptotically, where the desired accuracy of the algorithm is taken as a parameter, and as this parameter gets smaller a longer sequence of elementary gates is constructed to achieve this level of accuracy.

\subsection{Classical Computation in Quantum Algorithms}

In classical computation we can imagine an algorithm or circuit mapping some $M$ discrete states to some $N$ discrete states, according to some function $f: \{0\dots M-1\} \to \{0\dots N-1\}$. (say $M = 2^m,\ N = 2^n$ where $m$ and $n$ are the number of physical wires leading in and out of the circuit) For example we could define the logical conjunction or AND map which maps pairs of bits $\{00, 01, 10, 11\}$ to single bits $\{0, 1\}$: 
\begin{align*}
\text{AND}(x) = \begin{cases}
1 & \text{if\ } x = 11\\
0 & \text{otherwise}
\end{cases}
\end{align*}

Given such a map $f$ we can define a linear operator by extending linearly on the computational basis: $A_f\ket{i} = \ket{f(i)}$, giving a matrix whose columns are all computational basis vectors. For example our logical conjunction becomes:

\[
A_\text{AND} = \left[\begin{matrix}
1&1&1&0\\
0&0&0&1
\end{matrix}\right]
\]

In quantum computation we require that all operations be reversible, unitary operations. This means that a matrix $A_f$ representing a classical computation $f$ will be available as a unitary operator if and only if $M=N$ and $f$ is invertible, i.e.\ if $f$ is a permutation. When $M = N = 2$ we only have two such permutation matrices:
\begin{align*}
I = \left[\begin{matrix}
1&0\\
0&1
\end{matrix}\right]
&&&
X = \left[\begin{matrix}
0&1\\
1&0
\end{matrix}\right]
\end{align*}

Maps that don't satisfy $M = N$ or $f$ invertible can still be represented as a permutation matrix, acting on $\mathbb{C}^{M\times N}$ as follows:
\[B_f(\ket{i}\otimes\ket{j}) = \ket{i}\otimes\ket{j+f(i) \mod N}\]

The inverse of this matrix is simply $\ket{i}\ket{j} \mapsto \ket{i}\ket{j-f(i) \mod N}$.

In the case of the binary AND map, $B_{\text{AND}}$ will be a $8\times8$ permutation matrix known as the Toffoli gate. It can be shown that compositions of Toffoli gates acting on some number of bits can be used to implement any $n$-qubit permutation matrix as an algorithm on some larger number of qubits $n+m$, so long as the extra $m$ qubits are initialized to known values.

Permutation matrices by themselves might not seem interesting, since they exclusively represent calculations that can be done in classical contexts, but in fact are crucial for many quantum algorithms, since they will act linearly on superposition states. For example if we consider the map $f(x) = x^2\mod 16$, then the constructed matrix $B_f$ acting on 8 qubits will of course distribute linearly over any linear combination of basis states including the following:
\begin{align*}
B_f(\ket{2}\ket{0}+\ket{5}\ket{0}) 
= B_f\ket{2}\ket{0} +& B_f\ket{5}\ket{0}
\\= \ket{2}\ket{4} +& \ket{5}\ket{9}
\end{align*}

Further manipulations or measurement of the result of such a transformation can enable many powerful quantum algorithms including Shor's period finding algorithm. This means that permutation matrices are an important topic in quantum computation, and a good deal of research has been and continues to be done to better understand how permutation matrices can be decomposed into efficient quantum algorithms.

\subsection{Phase and Amplitude}
Since we assume a very specific space $\mathbb{C}^n$ with a canonical/computational basis available, we can and often do talk directly about the coordinates of vectors in this space. We call each complex coordinate an \emph{amplitude}, and importantly in a composite space the amplitudes of a Kronecker product are the coordinates \emph{after} expanding the product not before. For example the Kronecker product $\ket{+} \otimes \ket{-}$ as a vector looks like the following:
\[\ket{+} \otimes \ket{-} = \left[\begin{matrix}
\frac{1}{2} \\
-\frac{1}{2} \\
\frac{1}{2} \\
-\frac{1}{2} \\
\end{matrix}\right]\]
This would have amplitudes $1/2$, $-1/2$, $1/2$, and $-1/2$ in the computational basis.

When comparing different states or coordinates within a state we often find that their coordinates are equal in magnitude, but have different complex argument. In such cases we can talk about the phase differences between the states or the coordinates of each state, to understand the state better. If two states are multiples of each other, specifically a unit complex multiple, e.g.\ $\ket{\psi} = e^{i\theta}\ket{\phi}$, then we say that they differ by a global phase factor $e^{i\theta}$, and if two amplitudes of states are unit complex multiples of each other, e.g. $\ket{\psi} = \ket{0}/\sqrt{2} + e^{i\theta}\ket{1}/\sqrt{2}$ vs. $\ket{\phi} = \ket{0}/\sqrt{2} + \ket{1}/\sqrt{2}$ then we say their $\ket{1}$ amplitudes differ by a relative phase factor $e^{i\theta}$.

Interestingly, global phase does not have any physical meaning, and exists only within the model, since the only predictions made by the model are the probabilities of each measurement, and global phase differences cannot affect the magnitude of any amplitude in either the state in question or any resultant states after performing computation. For example if we have two states $\ket{+}$ and $-\ket{+}$ and apply the Hadamard matrix $H$ to each we would get $\ket{0}$ and $-\ket{0}$ respectively, both before and after they have identical behaviour when measured.

Relative phase differences on the other hand, while not directly affecting measurement, can affect the course of calculation, producing states that are very different when measured. For example $\ket{+}$ and $\ket{-}$ differ only by relative phase $-1$ in the $\ket{1}$ amplitude, but after applying $H$ we get $\ket{0}$ and $\ket{1}$ respectively, different computational basis vectors, which we have assumed to be distinct when measured.

Amplitude and relative phase differences can be generalized to arbitrary basis if one needs, whereas global phase differences are basis independent. Global phase differences are much more relevant to us than relative phase anyway, so we won't need any generalized notion of amplitude or relative phase. [in fact I don't think we use the basic version either!]

By ignoring global phase differences, we can remove a degree of freedom from the state space being discussed, but we can also remove a degree of freedom from any sets of unitary matrices being explored, since any scalar multiple of the identity will have no meaningful effect on computation, so for example $X$ and $-X$ can be considered equivalent:
\[-X = -\left[\begin{matrix}0 & 1 \\ 1 & 0\end{matrix}\right]= \left[\begin{matrix}0 & -1 \\ -1 & 0\end{matrix}\right]\]
Given a subgroup of the unitary matrices $G \leq U(n)$ we can make this equivalence explicit, by identifying $U(1)$ with scalar multiples of the identity in $U(n)$, and writing $G/U(1)$, which is understood to be the group quotient $G/(G \cap U(1))$, where $G \cap U(1)$ must be a normal subgroup of $G$ since it consists only of scalars. This lets us state formally that in the above example $X$ and $-X$ would belong to the same coset in $U(n)/U(1)$.
\subsection{Pauli Matrices}
A useful family of matrices are the Pauli matrices, which in the $2\times2$ qubit case are the $X$ and $Z$ matrices discussed in previous examples:
\begin{align*}
X = \left[\begin{matrix}
0&1\\
1&0
\end{matrix}\right]
&&&
Z = \left[\begin{matrix}
1&0\\
0&-1
\end{matrix}\right]
\end{align*}

These can be generalized to $n\times n$ matrices with the following effect on the computational basis:
\begin{align*}
X_n\ket{i} = \ket{i+1\mod n}
&&&
Z_n\ket{i} = \omega_n^i\ket{i}
\end{align*}

Powers of $X_n$ form a cyclic group of permutation matrices of order $n$. Visualizing the computational basis vectors in a circle, powers of $X_n$ rotate the circle, whereas powers of $Z_n$ resemble harmonics on that circle.

Observe that these matrices do not commute, but that $Z_nX_n = \omega_nX_nZ_n$:
\begin{align*}
	Z_nX_n\ket{i}
	=& Z_n\ket{i+1\mod n}
	\\=& \omega_n^{i+1}\ket{i+1\mod n}
	\\=& \omega_n^{i+1}X_n\ket{i}
	\\=& \omega_nX_nZ_n\ket{i}
\end{align*}

From this it can be seen that all elements of the Pauli group will be of the form $\omega_n^iX_n^jZ_n^k$, giving the Pauli group an order of $n^3$.

This group is an interesting finite group in quantum computation, first brought to attention since $X_2$ and $Z_2$ along with $Y  = iX_2Z_2$ have physical meaning in quantum mechanical description of electron spin. Further, these 3 matrices each transform the state space of a qubit in a very geometrically convenient way, each along a different pair of axes in the Bloch sphere. [either describe the Bloch sphere specifically for this statement, or make this statement more vague] The Pauli group of generalized Pauli matrices provide similar algebraic power, being generated by 2 gates that might also have physical meaning, while still providing some level of transformation of quantum states along all axes.
[this is a huge part of my intuition for why we start with the Pauli group, but this explanation is a mess. I don't want to get into the full details of how unitary operators form a sphere in Mat(n, C), although I could, if I brought back the ``displacement operators as basis for general operators'', I think $U(n)$ is literally the unit sphere in this basis? It's a big aside for an intuition that isn't directly used here, since what we tend to talk about instead is the power of $H_2$ and $H_3$.]

The convenience of the $Y_2$ matrix along with the identity $D_{x, z}^k = D_{kx,kz}$ motivate the definition of the displacement operators as seen in \cite{pi-over-eight}:
\[D_{x, z} = \omega^{xz}_{2n}X^xZ^z\]
These displacement operators are equivalent to the generalized Pauli matrices up to global phase, but generate a group of twice the size called the Weyl-Heisenberg group:
\[H(n) = \{\omega_{2n}^iX^jZ^k\}\]

The normaliser of the Weyl-Heisenberg group is called the Clifford group, and must be at least as powerful as the Pauli group, while also providing, among others, the discrete Fourier transform:
\[H_n = \frac{1}{\sqrt{n}}\sum_{i,j} \omega_n^{ij}\ket{i}\bra{j}\]
The Clifford group contains arbitrary scale factors, making it uncountably infinite, but when removing these scale factors it turns out to be another powerful finite group, making its generator a very useful place to start when considering possible elementary gate sets for a quantum computer.

[describe the SWAP operation, which is always Clifford]


\section{Notation for Mixed Systems}
[make sure that I am actually using this notation, also maybe throw some references in even though this is mostly standard]

A very ergonomic notation for gates in quantum algorithms is to subscript the gate with relevant information about that gate such as the dimension of the object or system on which it acts, the index of the individual object or part of the composite system on which it acts, or the computational basis states that it affects.

Many discussions of quantum algorithms vary only one property of a given quantum gate at a time, making this notation unambiguous. Unfortunately we must have gates acting on distinct objects of a composite system, each with a dimension that could be different, which requires us to take extra care. Lest we lose the ergonomics of putting all the necessary information in small and descriptive subscripts, we continue to place variables in these subscripts, but avoid numerals unless the meaning is clear. We will now enumerate in advance the exact index sets that could be used for a variable in a subscript, a preferred letter in the alphabet for representing such a variable, and what that subscript means when this variable is used.

In composite quantum systems we define $N$ to be the number of computational basis states, and either $n$ to be the number of individual objects in the composite system, or $n$ to be the number of qubits and $m$ to be the number of qutrits.[we don't use this $n$ vs $m$ discussion, all qubit qutrit discussions are in 2 or 3 objects] For example we could use these variables to index the Quantum Fourier Transform $\text{QFT}_N$ or a general unitary matrix $U_N$ acting on $N$ computational basis states, or $H^{\otimes n}$ for the Kronecker product of the $H$ matrix with itself $n$ times.

In quantum systems of a single finite-dimensional object we instead call the dimension of this object $n$, and index each of the computational basis states $0$ through to $n-1$, represented by variables such as $i$, $j$, and $k$. For example we will frequently distinguish between $X_2$, $X_3$, and generally $X_n$ in the context of a single quantum object. This is the one case where we will use both numerals and variables for subscripts, since we very often want to say specific things about the $n=2$ and $n=3$ cases.

Note that we change the meaning of $n$ based on context, for the sole purpose of reminding ourselves of the cyclic group formed by the set $\mathbb{Z}_n = \mathbb{Z}/n\mathbb{Z}$ equipped with addition, a very familiar object which appears repeatedly in finite groups of quantum gates acting on single quantum objects.
[do we actually talk about single objects outside of the preliminary discussion of Pauli matrices? If the previous subsection counts then we should move this even earlier!]

When we want to talk about an individual object in a composite system, we index each object $1$ through to $n$, (or $1$ through to $n+m$) and use variables such as $p$, $q$, and $r$ to represent such an index. We then define $d_p$ to be the dimension of the object indexed by $p$. This allows us to extend some unitary matrix $G_d$ acting on a single quantum object of dimension $d$ to a unitary matrix $G_p$ acting on some composite system satisfying $d_p = d$. Written as a Kronecker product, $G_p$ looks like the following:
\[G_p = I_{d_1}\otimes \dots \otimes I_{d_{p-1}} \otimes G_d \otimes I_{d_{p+1}} \otimes \dots \otimes I_{d_n}\]

When indexing computational basis states in a composite system, we use the integers $0$ through to $N-1$ much like in single systems, [unless I removed that paragraph/notation] and variables such as $i$, $j$, and $k$ to represent such indeces. This is particularly useful for the special class of permutation matrices that represent a transposition, where we can write $S_{i,j}$ to represent the matrix that exchanges the computational basis states $\ket{i}$ and $\ket{j}$, while leaving all others unchanged.

When we introduce a variable such as $i$, satisfying $0 \leq i \leq N-1$, we assume implicitly that the sequence of digits $i_1i_2\dots i_p \dots i_n$ exists and that each variable $i_p$ is understood to refer to the corresponding digit in this sequence. We also identify all such integers $0\dots N-1$ with their corresponding sequences of digits whenever we discuss the computational basis states.

When we have a unitary matrix $U$ acting on a composite system with $N$ computational basis states, and a set $c \subseteq \{0\dots N-1\}$, our final subscript notation will be to define the controlled operation $C_c(U)$ which has the following action on the computational basis:
\[C_c(U)\ket{i} = \begin{cases}
	U\ket{i} & \text{if\ }i \in c \\
	\ket{i} & \text{if\ }i \notin c
\end{cases}\]
Note that in order for $C_c(U)$ to be unitary, it is sufficient that the image $U\ket{i}$ of any computational basis vector $\ket{i}$ satisfying $i \in c$ be a linear combination of basis vectors $\ket{j}$ themselves satisfying $j \in c$. For example if we have a gate $G_p$ acting only on object $p$ of the system, then $C_c(G_p)$ will be unitary whenever $c$ is a set whose defining predicate $i_1\dots i_n \in c$ does not depend on the digit $i_p$.

When we have a condition 

Finally when $G_d$ is a unitary matrix acting on a $d$-dimensional object, and $G_p$ is the corresponding matrix acting on object $p$ of a composite system, it is sometimes useful for the set $c$ to impose a constraint on all but $d$ of the computational basis states of the composite system, in which case we can write $C_c(G_d)$ instead of $C_c(G_p)$.