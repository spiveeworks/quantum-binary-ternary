% chap1.tex (Chapter 1 of the thesis)

% Note that the text in the [] brackets is the one that will
% appear in the table of contents, whilst the text in the {}
% brackets will appear in the main thesis.

%[script style reduce fraction size]

%[quantikz]

%\section{Quantum Mechanics}

\chapter[PRELIMINARIES]{Preliminaries}
\section{Algebra of Unitary Matrices}
In quantum mechanics it will turn out crucial to have a strong theory of unitary operations acting linearly on complex-valued objects, so to that end we shall define these concepts and their notation here. [And sketch some theorems?]
\subsection{Hilbert Spaces}
Cartesian coordinates provide a powerful abstract way of reasoning about physical space as the combination of 3 variables, or conversely a way of visualizing combinations of variables as planes or volumes within a physical space. Hilbert spaces are a description which abstracts the Cartesian coordinate system even further, describing a much larger class of mathematical objects with similar geometric properties, including the space of possible states that a quantum object can take.

The abstract definition of a Hilbert space describes the following mathematical objects:
\begin{itemize}
	\item A set of `scalars', either $\mathbb{R}$ or $\mathbb{C}$
	\item A (nonempty) set of `vectors' $\mathbb{H}$
	\item A binary operation of vector addition: $\mathbb{H} \times \mathbb{H} \to \mathbb{H}$, written $u + v$ where $u$, $v$ are the vectors being added.
	\item An operation that `scales' vectors by a scalar: $\mathbb{K} \times \mathbb{H} \to \mathbb{H}$, simply written $av$ where $a$ is the scalar and $v$ is the vector
	\item An operation called the inner product, yielding a scalar for each pair of vectors: $\mathbb{H} \times \mathbb{H} \to \mathbb{K}$, written for now as $(u, v)$ where $u$, $v$ are the vectors involved.
	\item Well defined limits of any Cauchy sequence in $\mathbb{H}$, written $\lim_{n \to \infty} v_n$ where $(v_n)_{n=1}^\infty$ is the Cauchy sequence
\end{itemize}
Note that since $\mathbb{H}$ is non-empty, it must contain a `zero vector' acquired by scaling any element of $\mathbb{H}$ by 0. This vector is itself written as 0. When these objects are available, we say that $\mathbb{H}$ is a Hilbert space if it further satisfies the following algebraic properties, for any $a, b\in \mathbb{K}$, $u,v,w \in \mathbb{H}$:
\begin{itemize}
	\item vector associativity: $u + (v + w) = (u + v) + w$
	\item vector commutativity: $u + v = v + u$
	\item scalar identity: $1v = v$
	\item scalar associativity: $a(bv) = (ab)v$
	\item vector distribution: $a(u + v) = au + av$
	\item scalar distribution: $(a+b)v = av + bv$
	\item conjugate symmetry of inner products: $(u, v) = (v, u)^*$ (where $a^*$ is the complex conjugate of $a$)
	\item right linearity: $(u, v+w) = (u, v) + (u, w)$
	\item positive definite: $(v, v)$ real, and strictly positive whenever $v \neq 0$
\end{itemize}
Additionally Cauchy sequences must actually converge to their limits, a property whose formal definition and applications are beyond the scope of this thesis.

The primary Hilbert spaces discussed in this thesis are the sets of complex valued column vectors:
\[\mathbb{C}^n = \left\{\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]\ |\ a_0, a_1, \dots a_n \in \mathbb{C}\right\}\]
Adding and scaling of vectors of course take the usual definitions:
\[
c_0\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]
+
c_1\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
=
\left[\begin{matrix} c_0a_0+c_1b_0\\c_0a_1+c_1b_1\\\vdots\\c_0a_n+c_1b_n\end{matrix}\right]
\]
Inner products take a fairly natural definition:
\[
\left(
\left[\begin{matrix} a_0\\a_1\\\vdots\\a_n\end{matrix}\right]
,
\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
\right)
=
\left[\begin{matrix} a_0^*&a_1^*&\cdots&a_n^*\end{matrix}\right]
\left[\begin{matrix} b_0\\b_1\\\vdots\\b_n\end{matrix}\right]
= \sum_{i=0}^{n-1} a_i^*b_i
\]
Inner products introduce a concept of orthogonality to a space of vectors, we say two vectors $u, v$ are orthogonal when $(u, v) = 0$.
\[
\left(\left[\begin{matrix}1\\0\end{matrix}\right],\left[\begin{matrix}0\\1\end{matrix}\right]\right) = 0
\]
In general if two column vectors $v_i$ and $v_j$ have coordinates 0 except for their $i$th and $j$th coordinate respectively, then $(v_i, v_j) = \delta_{ij}$, 1 when $i = j$ and 0 otherwise. This condition is what it means for the set $\{v_0 \dots v_{n-1}\}$ to be an orthonormal basis of the vector space $\mathbb{C}^n$. It is a fundamental result of Hilbert spaces that if a countable set of vectors $v_i \in \mathbb{H}$ span the whole space, that is if every vector $w \in \mathbb{H}$ is an infinite linear combination of the vectors $v_i$, then $\mathbb{H}$ has an orthonormal basis. If this orthonormal basis is finite then we say that $\mathbb{H}$ is finite dimensional, and can observe any vector $v \in \mathbb{H}$ as analogous to the following column vector in $\mathbb{C}^n$:
\[\left[\begin{matrix}
	(v_0, v)\\
	(v_1, v)\\
	\cdots\\
	(v_n, v)
\end{matrix}\right]\]
Notice that the column vectors corresponding to the basis vectors $v_i$ have coordinate 0 everywhere except for their $i$th co-ordinate which is 1. Further the co-ordinates of these column vectors are exactly the co-ordinates of $v$ in $\mathbb{H}$:
\[\left(v_i, \sum_{j=0}^{n-1} a_jv_j\right) = \sum_{j=0}^{n-1} a_j\left(v_i, v_j\right) = a_i\]
Finally the inner products of vectors $u, v \in \mathbb{H}$ are exactly the inner products of the corresponding column vectors:
\begin{align*}
	(u, v) &= \left(\sum_{i=0}a_iv_i, \sum_{j=0} b_jv_j\right)
	\\&= \sum_{j=0}b_j \left(\sum_{i=0}a_iv_i, v_j\right)
	\\&= \sum_{j=0}b_j \left(v_j, \sum_{i=0}a_iv_i\right)^*
	\\&= \sum_{i,j=0}a_i^*b_j \left(v_j, v_i\right)^*
	\\&= \sum_{i,j=0}a_i^*b_j \delta_{ij}
	\\&= \sum_{i=0}a_i^*b_i
	\\&= \sum_{i=0}(v_i, u)^*(v_i, v)
	\\&= \left(
	\left[\begin{matrix}
		(v_0, u)\\
		(v_1, u)\\
		\cdots\\
		(v_n, u)
	\end{matrix}\right]
	,
	\left[\begin{matrix}
		(v_0, v)\\
		(v_1, v)\\
		\cdots\\
		(v_n, v)
	\end{matrix}\right]
	\right)
\end{align*}
[could prove conjugate linearity earlier, or stop proving linear algebra results altogether!!]

In this sense any finite dimensional Hilbert space is geometrically equivalent to the complex vector space $\mathbb{C}^n$ where $n$ is the size of any orthonormal basis of $\mathbb{H}$.

As we shall describe in Chapter 1, quantum computation most commonly acts on Hilbert spaces that are finite dimensional, and while the specific space being considered can be important in practical contexts, this thesis is concerned with the theoretical and therefore only needs to understand the complex vector space $\mathbb{C}^n$.

One caveat to this equivalence is the concept of basis-independence, that many concepts in linear algebra are made useful by the fact that they give the same result regardless of which basis is used for the space in question. To show that something is basis independent, it is sufficient and often convenient to define it directly in terms of the inner product and vector operations, rather than the coordinate system induced by a given basis. As an example the $\ell^2$ norm in $\mathbb{C}^n$ does not depend on the basis used:
\[\norm{v}_2 = \sqrt{(v, v)} = \sqrt{\sum_{i=0}^{n-1}\ord{v_i}^2}\]
This cannot be done for any other $\ell^p$ norm.

\subsection{Dirac Notation}
There are a number of notations available for reasoning about linear algebra and vector spaces, but we shall see [do we?] that in order to reason about quantum algorithms we will rely heavily on the following techniques:
\begin{itemize}
	\item linear operators defined in terms of inner products
	\item change of basis via unitary operators
	\item change of index set when summing vectors
\end{itemize}
For this collection of techniques the Dirac notation for vectors and linear operators is particularly well suited.

The main feature of Dirac notation is the ket, where a bar and angle bracket are used to distinguish a symbol as representing a vector: $\ket{v}$, $\ket{*}$, $\ket{0}$, $\ket{i}$, etc.

The most common vectors used are the basis vectors, and in Dirac notation it is natural to use the index of each basis vector as the symbol \textit{for the vector itself}:
\begin{align*}
	\ket{0} = \left[\begin{matrix}
		1\\
		0
	\end{matrix}\right]
	&&&
	\ket{1} = \left[\begin{matrix}
		0\\
		1
	\end{matrix}\right]
\end{align*}

The inner product of two vectors $(\ket{u}, \ket{v})$ is normally written $\braket{u}{v}$, called a `bra-ket'. This is inspired by the notation $\langle u, v\rangle$, used before Dirac, but allows an elegant representation of co-vectors. Co-vectors map a vector to a scalar, and in an inner product space every vector has a natural co-vector defined using the inner product. In Dirac notation we can simply omit the ket from an inner product to notate a co-vector:
\begin{align*}
	\bra{u} &: \mathbb{C}^n \to \mathbb{C}
	\\\bra{u} &\equiv \ket{v} \mapsto \braket{u}{v}
\end{align*}

\subsection{Linear Operators}
If $U$ and $V$ are Hilbert spaces, (or generally vector spaces) then a map $A: U \to V$ is a linear operator if it satisfies the following in general:
\[A(a\ket{u} + b\ket{v}) = aA\ket{u} + bA\ket{v}\]
If we have vectors $\ket{u} \in U$ and $\ket{v} \in V$ then we can define a simple linear operator using the inner product in $U$:
\[\ket{v}\bra{u} \equiv \bra{u'} \mapsto \braket{u}{u'}\ket{v}\]
The fact that this is linear is a consequence of the inner product being linear in its second argument, one of the defining properties of Hilbert spaces.

Note that if we reverse the notation for scaling a vector we get the expression $\ket{v}\braket{u}{u'}$, which looks very similar to the application $\ket{v}\bra{u}(\ket{u'})$, again equivocating the map with the object it produces, just as was done in defining co-vectors.

As we have discussed, when dealing with any finite dimensional Hilbert space we can assume the space is equivalent to some complex vector space $\mathbb{C}^n$, in which case matrix representations of a linear operator become available. We derive matrix representation explicitly, since the relevant technology is readily available from Dirac notation. First observe that linear operators $A$ say, are determined uniquely by the image of each basis vector $\ket{j}$, say $A\ket{j} = \ket{v_j}$. First evaluate an arbitrary application of $A$:
\begin{align*}
	A\left(\sum_{j=0}^{n-1}u_j\ket{j}\right)
	&= \sum_{j=0}^{n-1}u_jA\ket{j}
	\\&= \sum_{j=0}^{n-1}u_j\ket{v_j}
\end{align*}
Then this turns out to be the same result that we get from applying the following:
\begin{align*}
	\left(\sum_{j=0}^{n-1}\ket{v_j}\bra{j}\right)\left(\sum_{i=0}^{n-1}u_i\ket{i}\right)
	&= \sum_{i=0}^{n-1}\sum_{j=0}^{n-1}u_i\braket{j}{i}\ket{v_j}
	\\&= \sum_{j=0}^{n-1}u_j\ket{v_j}
\end{align*}
Since these two maps are equal on their whole domain, they are the same:
\[A = \sum_{j=0}^{n-1} \ket{v_j}\bra{j}\]
Further if the codomain is also finite dimensional then we can repeat this process. First note that the outer product notation $\ket{v}\bra{u}$ is linear on $\ket{v}$, regardless of which vector $\ket{u'}$ it is applied to:
\[(a\ket{v_1}+b\ket{v_2})\braket{u}{u'} = a\ket{v_1}\braket{u}{u'}+b\ket{v_2}\braket{u}{u'}\]
Now suppose the image vectors $\ket{v_j}$ are in $\mathbb{C}^m$ and have coordinates themselves, $\ket{v_j} = \sum_i^{m-1} a_{ij}\ket{i}$, then:
\begin{align*}
	A = \sum_{i,j} a_{ij}\ket{i}\bra{j}
\end{align*}
That is, all linear operators are a linear combination of outer products between basis vectors, making these outer products a basis for the (Hilbert) space of linear operators between finite dimensional Hilbert spaces. We have derived a way of representing linear operators $\mathbb{C}^n \to \mathbb{C}^m$ as an array of $n \times m$ scalar coordinates, which is the exact structure we want from a matrix representation, and matrix multiplication appears directly from composing the corresponding maps:
\[\left(\sum_{i,j}a_{ij}\ket{i}\bra{j}\right)\left(\sum_{j',k}b_{j'k}\ket{j'}\bra{k}\right) = \sum_{i,k}\left(\sum_j a_{ij}b_{jk}\right)\ket{i}\bra{k}\]

For example any linear operator $A: \mathbb{C}^2 \to \mathbb{C}^2$ can be written as follows:
\begin{align*}
	A = \left[\begin{matrix}
		a&b\\
		c&d
	\end{matrix}\right]
	=&\ 
	a\left[\begin{matrix}
		1&0\\
		0&0
	\end{matrix}\right]
	+
	b\left[\begin{matrix}
		0&1\\
		0&0
	\end{matrix}\right]
	+
	c\left[\begin{matrix}
		0&0\\
		1&0
	\end{matrix}\right]
	+
	d\left[\begin{matrix}
		0&0\\
		0&1
	\end{matrix}\right]
	\\=&\  a\ket{0}\bra{0}+b\ket{0}\bra{1}+c\ket{1}\bra{0}+d\ket{1}\bra{1}
\end{align*}

When defining a linear operator $L: \mathbb{C}^n \to \mathbb{C}^n$ we can define it as the `linear extension' of a more succinct map $\phi: \{\ket{i}\ |\ 0 \leq i < n\} \to \mathbb{C}^n$, by first defining $\ket{v_j} = \phi(\ket{j})$ and then setting $L = \sum_{j} \ket{v_j}\bra{j}$. This is simply a matrix whose columns are the vectors $\ket{v_j}$.
\subsection{The Hermitian Adjoint and Diagonalizable Matrices}
An aspect of linear algebra essential to quantum computation is diagonalization. We shall describe a number of properties that a square matrix can have, in terms of both its Hermitian Adjoint, and its eigenvalues, but first we define these concepts.

The Hermitian Adjoint of a linear operator $A$ (square or otherwise) is the unique linear operator $A^\dagger$ that satisfies $\braket{u}{v'} = \braket{u'}{v}$ whenever $\ket{v'} = A\ket{v}$, and $\ket{u'} = A^\dagger \ket{u}$. It turns out that the matrix representation of $A^\dagger$ is exactly the complex conjugate of the transpose of $A$, so this is taken as the definition of the Hermitian Adjoint of a matrix. As an example observe the following matrix $A$ and its Hermitian adjoint:
\begin{align*}
	A = \left[\begin{matrix}
		1 & 1+i\\
		0 & 1
	\end{matrix}\right]
	&&&
	A^\dagger = \left[\begin{matrix}
		1 & 0\\
		1-i & 1
	\end{matrix}\right]
\end{align*}

Note now that in the vector space $\mathbb{C}^2$ the Hermitian adjoint of a vector has the same behaviour as a co-vector:
\begin{align*}
	\ket{u}^\dagger\ket{v}
	=&\ 
	\left[\begin{matrix}
		a\\
		b
	\end{matrix}\right]^\dagger
	\left[\begin{matrix}
		c\\
		d
	\end{matrix}\right]
	\\=&\ 
	\left[\begin{matrix}
		a^*&b^*
	\end{matrix}\right]
	\left[\begin{matrix}
		c\\
		d
	\end{matrix}\right]
	\\=&\ a^*c+b^*d
	\\=&\ \braket{u}{v}
\end{align*}
This generalizes to any co-vector in any Hilbert space.

Associativity of the matrix product also lets us represent the outer product $\ket{u}\bra{v}$ using the Hermitian adjoint, as the matrix product $\ket{u}\ket{v}^\dagger$. This can be used to prove that
$(\ket{u}\bra{v})^\dagger = \ket{v}\bra{u}$

Now we move on to the eigenvector problem, which is the problem of finding a scalar $\lambda$ and non-zero vector $\ket{v}$ so that $A\ket{v}=\lambda\ket{v}$. Such a $\lambda$ is called an eigenvalue of $A$, and such a $\ket{v}$ is called the corresponding eigenvector of $A$. For example if A has a non-trivial null-space, then $\lambda=0$ will be an eigenvalue of $A$, and any vector in the null-space of $A$ will be a corresponding eigenvector of $A$: $A\ket{v} = 0\ket{v}$.

If $\ket{v}$ is normalized, then the matrix $B=\lambda\ket{v}\bra{v}$ will also satisfy the eigenvalue problem $B\ket{v}=\lambda\ket{v}$, and any vector orthogonal to $\ket{v}$ will be in the null-space of $B$. This means that if all of the eigenvectors of $A$ are orthogonal to eachother, then we can write $A$ as a sum of such $B$ vectors. This structure $A = \sum_i \lambda_i \ket{v_i}\bra{v_i}$ tells us that $A$ acts like a diagonal matrix on its eigenvalues, which we call the diagonal representation of $A$, and for this reason call any such $A$ `diagonalizable'.

As an example, the following matrix $Z$ is already diagonal and so has a straight-forward diagonal representation:
\begin{align*}
	Z = \left[\begin{matrix}
		1&0\\
		0&-1
	\end{matrix}\right] = 1\ket{0}\bra{0} - 1\ket{1}\bra{1}
\end{align*}

Having orthogonal eigenvectors is closely related to the Hermitian adjoint through a class of matrices called `normal' matrices, where a matrix $A$ is normal if it satisfies $A^\dagger A = AA^\dagger$. A major result of linear algebra called the spectral theorem of normal matrices, is that a matrix is diagonalizable if and only if it is normal. What follows is a list of different classes of normal operator, each defined by a property of the matrix, and by an equivalent property of all of its eigenvalues:
\begin{align*}
	\text{Hermitian matrix:\ }&&& A^\dagger = A & \iff& \lambda \in \mathbb{R} \\
	\text{Unitary matrix:\ }&&& A^\dagger = A^{-1} & \iff& \ord{\lambda}=1 \\
	\text{Positive normal:\ }&&& \bra{v}A\ket{v} \in \mathbb{R}, \geq 0 & \iff& \lambda \in \mathbb{R}, \geq 0 \\
	\text{Positive definite normal:\ }&&& \bra{v}A\ket{v} \in \mathbb{R}, > 0 & \iff& \lambda \in \mathbb{R}, > 0 \\
	\text{Projection matrix:\ }&&& A^2 = A & \iff& \lambda \in \{0, 1\} \\
\end{align*}

Finally, if $P$ is a polynomial then we can evaluate it on a square matrix $A$ as well, and it is easy to show that this polynomial `applies itself' to the eigenvalues of $A$, i.e.\ if $A\ket{v} = \lambda\ket{v}$ then $P(A)\ket{v} = P(\lambda)\ket{v}$.

Now any analytic function will be a limit of some polynomials, most notably the exponential:
\[e^x = \sum_n^\infty \frac{x^n}{n!}\]
This motivates us to apply such analytic functions directly to normal matrices, simply by applying it to each eigenvalue:
\begin{align*}
	f\left(\sum_i \lambda_i\ket{i}\bra{i}\right)
	&= \lim_{L \to \infty} \sum_{n=0}^L P_i\left(\sum_i \lambda_i\ket{i}\bra{i}\right)
	\\&= \lim_{L \to \infty} \sum_{n=0}^L \sum_i P_i(\lambda_i)\ket{i}\bra{i}
	\\&= \sum_i \lim_{L \to \infty} \left(\sum_{n=0}^L  P_i(\lambda_i)\right)\ket{i}\bra{i}
	\\&= \sum_i f(\lambda_i)\ket{i}\bra{i}
\end{align*}

This can be very useful for solving certain matrix equations such as $A^2 = B$ having the solution $A = \sqrt{B}$. It is also useful for mapping between Hermitian and unitary matrices with the map $U = e^{iH}$.

\subsection{Kronecker Products}
We have made explicit the way in which vectors, co-vectors, and linear operators are represented as matrices, that is as arrays of complex numbers. An operation that is useful for all of these objects is the tensor product, and while the tensor product can be described in the abstract as an operation between Hilbert spaces, the only cases of interest here are complex-valued matrices, whose tensor products are described by a much more concrete operation called the Kronecker product.

The Kronecker product has a fairly simple definition, if $A$ is an $m_1$ by $n_1$ matrix with elements $a_{ij}$, and $B$ is an $m_2$ by $n_2$ matrix with elements $b_{ij}$ then the Kronecker product $A \otimes B$ will be an $m_1m_2$ by $n_1n_2$ matrix and in block matrix form will look like the following:
\begin{align*}
	A &= \left[\begin{matrix}
		a_{00} & a_{01} & \dots & a_{0n_1}\\
		a_{10} & a_{11} & \dots & a_{1n_1}\\
		\vdots & \vdots & \ddots & \vdots\\
		a_{m_10} & a_{m_11} & \dots & a_{m_1n_1}
	\end{matrix}\right]
	\\\implies A\otimes B &= \left[\begin{matrix}
		a_{00}B & a_{01}B & \dots & a_{0n_1}B\\
		a_{10}B & a_{11}B & \dots & a_{1n_1}B\\
		\vdots & \vdots & \ddots & \vdots\\
		a_{m_10}B & a_{m_11}B & \dots & a_{m_1n_1}B
	\end{matrix}\right]
\end{align*}
Written more compactly, if $A\otimes B = C$ has elements $c_{m_2i_1 + i_2,n_2j_1+j_2}$, where $i_1$ is less than $m_1$, etc.\ then these elements of $C$ are exactly the products of elements of $A$ and $B$:
\[c_{m_2i_1 + i_2,n_2j_1+j_2} = a_{i_1j_1}b_{i_2j_2}\]

If $A$ is a vector $\ket{u}$ and $B$ is a co-vector $\bra{v}$ then their Kronecker product will be exactly the matrix product $AB$ which is exactly the outer product $\ket{u}\bra{v}$:
\[
A \otimes B =
\left[\begin{matrix}
	a_0\\a_1\\\vdots\\a_m
\end{matrix}\right]
\otimes
\left[\begin{matrix}
	b_0&b_1&\dots&b_n
\end{matrix}\right]
=
\left[\begin{matrix}
	a_0b_0 & a_0b_1 & \dots & a_0b_n\\
	a_1b_0 & a_1b_1 & \dots & a_1b_n\\
	\vdots & \vdots & \ddots & \vdots\\
	a_mb_0 & a_mb_1 & \dots & a_mb_n
\end{matrix}\right]
\]
On the other hand if $A$ and $B$ are both vectors, in $\mathbb{C}^{m_1}$ and $\mathbb{C}^{m_2}$ respectively, then their Kronecker product will be a vector in the larger space $\mathbb{C}^{m_1\times m_2}$, and in particular if $A$ and $B$ are canonical basis vectors $\ket{j_1}$ and $\ket{j_2}$ then their tensor product will be another canonical basis vector, $\ket{m_2j_1 + j_2}$. This allows the whole space $\mathbb{C}^{m_1 \times m_2}$ to be spanned by Kronecker products of $\mathbb{C}^{m_1}$ and $\mathbb{C}^{m_2}$.

The Kronecker product satisfies the following algebraic properties, all of which follow directly from their relevant definitions:
\begin{itemize}
	\item $(A\otimes B)(C \otimes D) = (AC) \otimes (BD)$
	\item in particular $(A\otimes B)(\ket{u} \otimes \ket{v}) = (A\ket{u})\otimes (B\ket{v})$
	\item $I_{n_1} \otimes I_{n_2} = I_{n_1\times n_2}$
	\item $(A\otimes B)^\dagger = A^\dagger \otimes B^\dagger$
	\item in particular $(\ket{u}\otimes \ket{v})^\dagger = \bra{u} \otimes \bra{v}$
	\item if $\lambda$ is a scalar then $\lambda (A \otimes B) = (\lambda A) \otimes B = A \otimes (\lambda B)$
	\item in particular $\lambda (\ket{u} \otimes \ket{v}) = (\lambda \ket{u}) \otimes \ket{v} = \ket{u} \otimes (\lambda \ket{v})$
\end{itemize}
With these properties we can show that the Kronecker product of two unitary matrices will be unitary:
\begin{align*}
	(A \otimes B)(A \otimes B)^\dagger
	&= (A \otimes B)(A^\dagger \otimes B^\dagger)
	\\&= AA^\dagger \otimes BB^\dagger
	\\&= I \otimes I
	\\&= I
\end{align*}

Considering the degrees of freedom involved, we expect that in general there are many $m_1m_2 \times n_1n_2$ matrices that are not the Kronecker product of an $m_1 \times n_1$ matrix with an $m_2 \times n_2$ matrix. Written in block matrix form it is easy to tell whether or not a matrix $C$ is a Kronecker product $A \otimes B$, by checking that every block $B_{ij}$ is a scalar multiple of any non-zero block $B$. The entries of $A$ are simply these scalar multiples, solving $B_{ij} = a_{ij}B$, and hence $C = A \otimes B$. (and if there are no non-zero blocks then $C = 0 = 0 \otimes 0$)
\subsection{The Unitary Group}
So far we have discussed a number of useful algebraic properties exhibited by unitary matrices, but the most basic properties of unitary matrices are of course their forming a group:
\begin{itemize}
	\item Product of unitaries is unitary: ${(AB)}^\dagger = B^\dagger A^\dagger = B^{-1}A^{-1} = {(AB)}^{-1}$
	\item Matrix products are always associative (since composition of any operator will be associative)
	\item The identity matrix is unitary: $I^\dagger = I = I^{-1}$
	\item $A^{-1} = A^\dagger$ always exists and is unitary
\end{itemize}

Quantum computation deals extensively with unitary operators, and so subgroups formed by particular sets of unitary operators will be a frequent point of discussion. A subgroup of a group is simply a subset that is still a group when equipped with the same group operation. Our group operation is matrix multiplication, and so the group of $n \times n$ unitaries written $U(n)$ is actually a subgroup of the much larger general linear group $GL(n, \mathbb{C})$, of $n \times n$ matrices with non-zero determinant.

A further subgroup of the set of unitary matrices $U(n)$ is the symmetric group $\mathcal{S}_n$, which we represent as the set of $n \times n$ permutation matrices. A permutation is an invertible function mapping a set to itself, and given a permutation on the canonical basis $\{\ket{i}\ |\ 0 \leq i < n\} \to \{\ket{i}\ |\ 0 \leq i < n\}$, we can extend this linearly to an invertible matrix $\mathbb{C}^n \to \mathbb{C}^n$ whose columns are all canonical basis vectors. We call such a matrix a permutation matrix, and note that the adjoint of its outer product form $P = \sum_i \ket{\sigma(i)}\bra{i}$ will be $P^{\dagger} = \sum_i \ket{i}\bra{\sigma{i}}$, giving the following:
\begin{align*}
	PP^\dagger = \sum_{i,j} \ket{\sigma(i)}\braket{i}{j}\bra{\sigma(j)} = \sum_i \ket{\sigma(i)}\bra{\sigma(i)} = I
\end{align*}

The symmetric group of any finite set is always finite, and is typically the first group considered when exploring finite subgroups of $U(n)$.
\subsection{Group Quotients, Normal Subgroups, Normalisers}
[perhaps an example of a group and its normalizer, group isomorphisms, and semidirect products. excessive definition stuff can go in axiomata appendix]

Given an equivalence relation $\sim$ on a set $S$ it is often useful to consider equivalence classes, the subsets $[x] = \{y\ |\ x \sim y\}$, since these will be equal exactly when their representatives are equivalent, i.e. $[x] = [y] \iff x \sim y$. The set of all such equivalence classes is called the set quotient, and is written $S/\sim$. It provides a concrete object with the same structural properties that would come from `identifying' $x$ with $y$ whenever $x \sim y$. If $S$ is actually a group $G$ then its group operation sometimes induces an operation in the set quotient $[x][y] = [xy]$, but if this is well defined then we immediately find $[e]$ is a subgroup of $S$:
\[e \sim x \sim y \implies [e] = [e][e] = [x][y] = [xy] \implies e \sim xy\]
\[e \sim x \implies [x^{-1}] = [e][x^{-1}] = [x][x^{-1}] = [xx^{-1}] = [e] \implies e \sim x^{-1}\]
Further if $x \in [e]$ and $z \in S$ then $zxz^{-1} \in [e]$ as well:
\[e \sim x \implies [z][x][z^{-1}] = [z][e][z^{-1}] = [e] \implies e \sim zxz^{-1}\]

In general if a subgroup $H$ of $G$ satisfies this condition, that for any $h \in H$ and $g \in G$, $ghg^{-1} \in H$, then $H$ is said to be a normal subgroup of $G$. It turns out that not only is the equivalence class $[e]$ a normal subgroup, but whenever $H$ is a normal subgroup of $G$, the equivalence relation $x \sim y \iff xy^{-1} \in H$ gives a well defined group operation on the set quotient $G/\sim$. We call this group the group quotient $G/H$. When available, this is a powerful tool for understanding the structure of groups, since the group quotient $G/H$ may have convenient algebraic properties emerging from its corresponding equivalence relation.

As an example of a normal subgroup, take $G \subset GL(n, \mathbb{R})$ to be any group formed by matrix multiplication, and $H$ to be the set of scalars in $G$, that is the set $\{\lambda I\ |\ \lambda \in \mathbb{C}\} \cap G$. Since scalars are commutative, it is straight-forward that $g\lambda I g^{-1} = \lambda gg^{-1} = \lambda I \in H$.

The subgroup relation and the normal subgroup relation can be thought of as a partial order, and are often written as $\leq$ and $\trianglelefteq$ respectively, since both are transitive, reflexive, and anti-symmetric. While these relations are each transitive, we must be careful when mixing them; if $H$ is a normal subgroup of $N$, and $N$ is a subgroup of $G$, $H$ is not necessarily a normal subgroup of $G$, which is surprising when stated more succinctly as ``$H$ is normal in $N$ but is not normal in $G$''.

With this subtlety in mind we can find exactly such a group $N$, given any subgroup $H$ of $G$. We call this the normaliser of $H$ with respect to $G$, defined as the set $N_G(H) = \{g\ |\ g \in G,\ gHg^{-1} \subseteq H\}$. This is the maximal such $N$, any other $N'$ with $H$ normal will sit inside $N_G(H)$. Clearly the group quotient $N_G(H)/H$ will exist for any subgroup $H$ of $G$, which will be useless in the case that $N_G(H) = H$, but otherwise can be an interesting group, and can even inform interesting structure about the normaliser itself, making normalisers a useful and novel tool for exploratory algebraic work.

\section{Quantum Mechanics}

The theory of Quantum mechanics is foundational in contemporary understanding of physical systems, and features phenomena radically different to that of the classical world when the objects under consideration are able to achieve `coherence', e.g.\ when sufficiently small or low in temperature.

Quantum mechanics is deliberately `incomplete' in the sense that it doesn't make specific predictions about the behaviour of atoms, electrons, or anything else, but rather poses constraints and structure that other physical theories such as quantum electrodynamics can adopt.

Quantum computation is the study of how quantum phenomena can be used to effect computation. When designing an individual experiment or programmable quantum computer it is necessary to use a concrete, empirically verified model of physical phenomena, but the specific algorithms that can be run on a quantum computer turn out to be described sufficiently by the general theory of quantum mechanics alone. Since this thesis is concerned with algorithms alone, we shall summarize the description of quantum mechanics given in the ubiquitous textbook "Quantum Computation and Quantum Information" by Nielsen and Chuang \cite{textbook}. This will provide a foundation for explaining certain techniques and algorithms prominent in quantum computing, and their relationship to the questions we aim to explore.
\subsection{State Space}
The first postulate of quantum mechanics is that any closed quantum system can be modeled by some complex Hilbert space. While many of these Hilbert spaces are of infinite dimension, we can easily create finite dimensional Hilbert spaces by either trapping or ignoring the positions of individual particles. This allows us to model the state of most quantum computers as sitting inside $\mathbb{C}^n$, where $n$ is the dimension of whatever Hilbert space was originally being used as a model.

We call the specific elements of $\mathbb{C}^n$ state vectors whenever they represent a state that a quantum computer can take, allowing us to model quantum computation itself as an operation on these state vectors, and to describe the initial, final, and intermediate states of any given computation as a specific state vector.

The simplest possible quantum system is an isolated quantum bit, or qubit, which can be modelled with the 2-dimensional Hilbert space $\mathbb{C}^2$:
\[
\mathbb{C}^2 = \left\{\left[\begin{matrix}
a\\
b
\end{matrix}\right]\ \bigg|\ a, b \in \mathbb{C}\right\}
\]
\subsection{Evolution}
The second postulate of quantum mechanics can be stated in three equivalent ways: Given a closed quantum system with initial state vector $\ket{\phi}$ these 3 equivalent equations hold:
\[-i\hbar\frac{d}{dt}\ket{\phi} = H\ket{\phi}\]
\[\ket{\phi} = \exp\left(i \frac{tH}{\hbar}\right)\ket{\phi_0}\]
\[\ket{\phi} = U\ket{\phi_0}\]
Where $\hbar$ is the reduced Planck constant, which is a known scalar constant, $H$ is the Hamiltonian operator, a Hermitian operator associated with the energy present in the system, and $U$ is the unitary operator arising from fixing $t$ before exponentiating $itH/\hbar$.

This last form is the form of interest to us. Supposing that the energy of a system can be changed externally at precisely controlled times, effectively manipulating $H$ and $t$, the state of the system will be predictably transformed according to some sequence of unitary operators. We can therefore define a quantum algorithm to be a sequence of unitary operators which transform a quantum system in some computationally useful way.
\subsection{Measurement}
Although the quantum systems we are discussing are represented as having a continuous state space of possible state vectors, a fundamental (and titular!) peculiarity of quantum mechanics is that when observed, a quantum object will always appear to be in states that are elements of some orthonormal basis of the corresponding Hilbert space. What is more peculiar, and more well known of quantum mechanics, is that the state that is measured becomes the new state of the system.

It is impossible to perform any quantum computation without measuring the result at the end, so in any theoretical discussion it is required that at least one procedure is possible for measuring the system. Whichever procedure is used, we call the corresponding orthonormal basis the `computational basis`. When identifying the given Hilbert space $\mathbb{H}$ with the vector space $\mathbb{C}^n$, we make sure to also identify the computational basis of $\mathbb{H}$ with the canonical basis $\{\ket{i}\}$ of $\mathbb{C}^n$, so that measurement can always be done in this basis.

Suppose we have a quantum system with state vector $\ket{\phi}$, and let $\ket{\phi}$ equal the following:
\[\ket{\phi} = \sum_{i=0}^n a_i\ket{i}\]

When measured this system will appear to be in state $\ket{i}$ with probability $\ord{a_i}^2$, and after measurement will be in the state $a_i'\ket{i}$, where $a_i'$ is the normalized complex number $\frac{a_i}{\ord{a_i}}$.

In more algebraic terms this can be stated as follows:
When measured this system will appear to be in state $\ket{i}$ with probability $\ord{\braket{i}{\phi}}^2$, and after measurement will be in the state that comes from applying the projection matrix $\ket{i}\bra{i}$ and normalizing.

Given that the amplitudes of a state vector are now interpreted as being related to probabilities, it is important that state vectors have unit length:
\[\sum_{i=0}^n \ord{a_i}^2 = 1\]

[mention amplitudes earlier? are amplitudes the exact complex coordinates or just their complex modulus]

As an example consider the $\ket{-}$ state:
\[\ket{-}=\frac{1}{\sqrt{2}}\ket{0} - \frac{1}{\sqrt{2}}\ket{1}\]
When measured, this will have probability $1/2$ of appearing to be in the $\ket{0}$ state, after which it will in fact be in the $\ket{0}$ state, and probability $1/2$ of appearing to be in the $\ket{1}$ state, although in actual fact it will be in the $-\ket{1}$ state.
\subsection{Composite State Space}
Suppose that we have two quantum systems otherwise modelled as $\mathbb{C}^m$ and $\mathbb{C}^n$ respectively, which we allow to interact with each-other, but as a coupled system are closed. Intuitively if we measure both systems we should find the first in some computational basis state $\ket{i}$ and the second in some second computational basis state $\ket{j}$, giving a total of $m \times n$ different measurements. It turns out that to model this composite system we have to extend these $m \times n$ different pairs of states to the Hilbert space $\mathbb{C}^{m \times n}$, whose computational basis is now given by Kronecker products $\ket{i} \otimes \ket{j}$.

In general if we had of modeled the systems in states $\ket{v_1}$ and $\ket{v_2}$ then as a coupled system we model them as having the state $\ket{v_1} \otimes \ket{v_2}$. As an example consider the following pair of qubit states:
\begin{align*}
\ket{1} = \left[\begin{matrix}0\\1\end{matrix}\right]
&&&
\ket{+} = \left[\begin{matrix}\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
\end{align*}
Considered as a coupled system they would have the following state.
\[
\ket{1}\otimes \ket{+} = \left[\begin{matrix}0\\1\end{matrix}\right]
\otimes
\left[\begin{matrix}\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
=
\left[\begin{matrix}0\\0\\\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\end{matrix}\right]
\]

Note that we can concatenate kets to indicate the Kronecker product without ambiguity: $\ket{v_1}\ket{v_2} = \ket{v_1}\otimes\ket{v_2}$.

In classical computation we represent integers and other data as a sequence of small digits (binary digits most of the time) and in the same way we can represent integers as computational basis vectors $\ket{i_1}\ket{i_2}\dots\ket{i_n}$ in some large system made from composing individual `quantum digits', binary or otherwise.

[do we use $\ket{1101}$ notation for strings? I don't think so! in mixed system especially $2 \times 3$ it is helpful to write separate kets]

As was described in our original discussion of Kronecker products, not all quantum states in a composite system $\mathbb{C}^m\otimes\mathbb{C}^n$ are of the form $\ket{u}\otimes\ket{v}$. States that can't be represented as a single Kronecker product $\ket{u}\otimes\ket{v}$ are called entangled states. Entanglement is a very important feature of quantum mechanics and quantum computation that will be discussed on its own later in this document. For now though observe the Bell state:

\[\frac{1}{\sqrt{2}}\ket{0}\otimes\ket{0} + \frac{1}{\sqrt{2}}\ket{1}\otimes\ket{1} = \left[\begin{matrix}
\frac{1}{\sqrt{2}}\\
0\\
0\\
\frac{1}{\sqrt{2}}
\end{matrix}\right]\]
We can see this has two non-zero blocks, $1/sqrt{2}\ket{0}$ and $1/sqrt{2}\ket{1}$ respectively, which are not multiples of each-other.

[entanglement as basis independent property?]

Seeing as classical systems aren't treated as having any form of superposition, there is no classical analogy for entangled states. It is worth noting that in classical systems a composite system would be understood to have a state space that is the Cartesian product of two individual state spaces, and that the Cartesian product of an $m$-dimensional vector space with an $n$-dimensional vector space (equipped with point-wise vector addition say, giving the direct sum of two vector spaces) would have dimension $m+n$ rather than the $m\times n$ dimensional space given by composite quantum systems.
\subsection{Composite Evolution}
Just as with individual systems, an isolated composite system evolves according to some unitary operator after any discrete time step. In the composite system $\mathbb{C}^{mn}$ this could be any $mn$ by $mn$ unitary matrix, but the most commonly available operations are the controlled increment gate, which acts as a permutation on the computational basis $\ket{i}\ket{j} \mapsto \ket{i}\ket{i+j}$, along with operations that simply act on the individual systems that make up the composite, i.e.\ matrices that are the Kronecker product of two smaller unitary matrices.
\[U \otimes V = \sum_i \ket{u_i}\otimes\ket{v_i} \mapsto \sum_i (U\ket{u_i})\otimes(V\ket{v_i})\]

For example the operation $I \otimes H$ acting on the 2-qubit system $\mathbb{C}^{2\times 2}$, which applies $H$ to the second qubit and leaves the first qubit unchanged, will have the following matrix representation:
\[
\left[\begin{matrix}
1&0\\
0&1
\end{matrix}\right]
\otimes
\left[\begin{matrix}
\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}
\end{matrix}\right]
=
\left[\begin{matrix}
\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0&0\\
\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}&0&0\\
0&0&\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}\\
0&0&\frac{1}{\sqrt{2}}&-\frac{1}{\sqrt{2}}
\end{matrix}\right]
\]
The controlled increment operation in binary systems is called the controlled-not operation, and like any controlled-increment operation can not be represented as a Kronecker product of two individual operations. It has the following matrix representation:
\[
\left[\begin{matrix}
1&0&0&0\\
0&1&0&0\\
0&0&0&1\\
0&0&1&0
\end{matrix}\right]
\]

This notion of controlled operations is useful to generalize to any unitary operation acting on a smaller quantum system, which will be discussed in more detail in the context of both binary and ternary quantum systems later.

It is useful to note that the Kronecker product commutes with the outer product:
\[(\ket{u'}\bra{u})\otimes(\ket{v'}\bra{v}) = \ket{u'}\ket{v'}\bra{u}\bra{v}\]
This means that as with state vectors, the Kronecker product of canonical basis operators $\ket{i'}\bra{i}$ and $\ket{j'}\bra{j}$ will give all of the canonical basis operators acting linearly on the composite system $\mathbb{C}^{m \times n}$.
\subsection{Composite Measurement}
When measuring an individual system we gave the algebraic definition that state $\ket{\phi}$ would be measured in state $\ket{i}$ with probability $\ord{\braket{i}{\phi}}^2$, and would then be in the state that comes from applying the projection $\ket{i}\bra{i}$ and normalizing.

This definition as is would allow us to measure all of the individual components of the composite system at the same time, but it is actually much more powerful to measure one component by itself without otherwise disturbing the rest of the system. To that end suppose we have a composite quantum system whose state vector is the following:

\[\ket{\phi} = \sum_{i=0}^m\sum_{j=0}^n a_{ij}\ket{i}\ket{j}\]

Either the $\mathbb{C}^m$ or the $\mathbb{C}^n$ component of this system could be measured. Without loss of generality we shall describe measurement of the first state. The system will be in state $\ket{i}$ with probability:
\[P(\ket{i}) = \sum_{j=0}^n\ord{a_{ij}}^2\]
After measuring the system will be in the state that comes from applying $(\ket{i}\bra{i})\otimes I$ and then normalizing.

For example observe the following 2-qubit state vector $\ket{+}\ket{+}$:
\[\ket{+}\ket{+}=\frac{1}{2}\left(\ket{00}+\ket{01}+\ket{10}+\ket{11}\right)=\frac{1}{2}\left[\begin{matrix}
1\\
1\\
1\\
1
\end{matrix}\right]\]

If we measured the first qubit then we would observe $\ket{0}$ with probability $1/4+1/4 = 1/2$, and similarly $\ket{1}$ with probability $1/2$. Upon measuring $\ket{0}$ the overall state would be $\ket{0}\ket{+}$, whereas upon measuring $\ket{1}$ the overall state would be $\ket{1}\ket{+}$.

While this is straight-forward for states that are simply the Kronecker product of two states, measuring entangled states can be quite interesting. For example suppose we measured the first component of the Bell state $1/\sqrt{2}(\ket{00}+\ket{11})$. As before we would find it in state $\ket{0}$ with probability $1/2$, and $\ket{1}$ with probability $1/2$, but after measurement it would be in states $\ket{00}$ and $\ket{11}$ respectively. Measuring the first component of the system changed the second one! This is the basis of a number of novel features of quantum computing, quantum information theory, and quantum cryptography. [chuck in quantum teleportation]

[note somewhere that more than two systems continue naturally from these subsections]

\section{Quantum Computation}

With quantum mechanics and the associated concept of a quantum algorithm in place, we can begin to reason about the techniques of quantum computation. To this end we shall briefly discuss the computer science of probabilistic vs deterministic algorithms, as well as the group theory of different sets of unitary matrices, important concepts that lay the foundation of technical discussions in the field of quantum computation. With this in place we shall be prepared to discuss the state of the field when it comes to quantum computation on objects with more than two basis states.

\subsection{Probabilistic Algorithms}
In computer science there is a distinction between deterministic algorithms, and randomized/probabilistic algorithms, the latter often used as an implementation for a more abstract class of non-deterministic algorithms. In summary a deterministic algorithm is a sequence of exact steps that can be executed in order to compute a result, whereas a probabilistic algorithm relies on some source of random information to determine its control flow, meaning that the same input could result in many different outputs.

The advantage of probabilistic algorithms is that they can often avoid the worst-case performance of certain input states. (for example, sorting in ascending order a list that is already sorted in descending order) At an intuitive level such worst-case input states tend to exploit the specific order in which an algorithm explores its possible solutions, and so since a randomized algorithm has no single order in which it might explore solutions, such worst-case inputs do not exist.

On classical computers deterministic algorithms are often more natural, and even when a randomized algorithm is used it will likely use a pseudo-random number generator in place of a true random source of information. This is heavily contrasted with quantum computation, where measuring the state of a quantum object is inherently probabilistic, and physical implementations of quantum algorithms often introduce physical sources of error as well, as a result all quantum algorithms are essentially probabilistic.

One important consequence of this is that a quantum algorithm that appears to perform well might still perform better on a classical computer with a source of randomness; when comparing quantum with classical, we must consider the probabilistic algorithms of both.

A further distinction in computer science is made between Las Vegas and Monte Carlo algorithms, the former being algorithms that always yield a result, but have random run-time, and the latter being algorithms that may yield a result in fixed run-time, but have a random chance of failing or producing an incorrect result. Often an algorithm in one of these classes can be converted into the other, Las Vegas algorithms that repeatedly search for a solution can be modified to yield a false negative after a fixed number of attempts, and Monte Carlo algorithms that may produce a false negative can check their solution using a deterministic algorithm, and retry until a valid solution is found.

This means that in classical contexts this distinction is less important than that of deterministic vs probabilistic algorithms, since both classes can achieve similar things with the same resources. In quantum contexts however, algorithms are generally assumed to be Monte Carlo algorithms, since the sources of error discussed are sources of incorrect output, rather than sources of increased run-time. Further when a physical quantum computer is run, it is run repeatedly, often thousands of times, in order to determine the probability of each output, so that one can infer which output is the correct one.

\subsection{Classical Computation in Quantum Algorithms}

In classical computation we could imagine mapping some $M$ discrete states to some $N$ discrete states, (say $M = 2^m,\ N = 2^n$ where $m$ and $n$ are the number of physical wires leading in and out of the circuit) with some function $f: \{0\dots M-1\} \to \{0\dots N-1\}$. For example we could define the logical conjunction or AND map which maps pairs of bits $\{00, 01, 10, 11\}$ to single bits $\{0, 1\}$: 
\begin{align*}
\text{AND}(x) = \begin{cases}
1 & \text{if\ } x = 11\\
0 & \text{otherwise}
\end{cases}
\end{align*}

Given such a map $f$ we can define a linear operator by extending linearly on the computational basis: $A_f\ket{i} = \ket{f(i)}$, giving a matrix whose columns are all computational basis vectors. For example our logical conjunction becomes:

\[
A_\text{AND} = \left[\begin{matrix}
1&1&1&0\\
0&0&0&1
\end{matrix}\right]
\]

In quantum computation we require that all operations be reversible, unitary operations. This means that a matrix $A_f$ representing a classical computation $f$ will be available as a unitary operator if and only if $M=N$ and $f$ is invertible, i.e.\ if $f$ is a permutation. Naturally, such matrices are called permutation matrices, and can be recognized visually by their having a single 1 in each row and column, and 0 everywhere else. When $M = N = 2$ we only have two such permutation matrices:
\begin{align*}
I = \left[\begin{matrix}
1&0\\
0&1
\end{matrix}\right]
&&&
X = \left[\begin{matrix}
0&1\\
1&0
\end{matrix}\right]
\end{align*}

In general the set of $N\times N$ permutation matrices form a group isomorphic to the symmetric group on $N$ elements.

Maps that don't satisfy $M = N$ or $f$ invertible can still be represented as a permutation matrix, acting on $\mathbb{C}^{M\times N}$ as follows:
\[B_f(\ket{i}\otimes\ket{j}) = \ket{i}\otimes\ket{j+f(i) \mod N}\]

The inverse of this matrix is simply $\ket{i}\ket{j} \mapsto \ket{i}\ket{j-f(i)}$.

In the case of the binary AND map, $B_{\text{AND}}$ will be a $8\times8$ permutation matrix known as the Toffoli gate. It can be shown that any permutation matrix acting on $n$ qubits can be implemented as a sequence of Toffoli gates acting on $n+m$ qubits, so long as the extra $m$ qubits are initialized to known values.

Permutation matrices by themselves might not seem interesting, since they exclusively represent calculations that can be done in classical contexts, but in fact are crucial for many quantum algorithms, since they will act linearly on superposition states. For example if we consider the map $f(x) = x^2\mod 16$, then the matrix $B_f$ acting on 8 qubits will distribute linearly over any linear combination of basis states including the following:
\begin{align*}
B_f(\ket{2}\ket{0}+\ket{5}\ket{0}) 
= B_f\ket{2}\ket{0} +& B_f\ket{5}\ket{0}
\\= \ket{2}\ket{4} +& \ket{5}\ket{9}
\end{align*}

Further manipulations or measurement of the result of such a transformation can enable many powerful quantum algorithms including Shor's period finding algorithm. This means that permutation matrices are an important topic in quantum computation, and a good deal of research has been and continues to be done to better understand how permutation matrices can be decomposed into efficient quantum algorithms.

\subsection{Pauli Matrices}

A useful family of matrices are the Pauli matrices, which in the $2\times2$ qubit case are the $X$ and $Z$ matrices discussed in previous examples:
\begin{align*}
X = \left[\begin{matrix}
0&1\\
1&0
\end{matrix}\right]
&&&
Z = \left[\begin{matrix}
1&0\\
0&-1
\end{matrix}\right]
\end{align*}

These can be generalized to $n\times n$ matrices with the following action on the computational basis:
\begin{align*}
X_n\ket{i} = \ket{i+1\mod n}
&&&
Z_n\ket{i} = \omega_n^i\ket{i}
\end{align*}

Powers of $X_n$ form a cyclic group of permutation matrices of order $n$. Visualizing the computational basis vectors in a circle, powers of $X_n$ rotate the circle, whereas powers of $Z_n$ resemble harmonics on that circle. Further, noting the following identity:
\[x^n=1 \implies x = 1 \text{\ or\ } \sum_{i=0}^{n-1}x^i = 0\]
This can be used to derive diagonal matrices of the form $\ket{k}\bra{k}$ as a sum of powers of $Z_n$:
\begin{align*}
\sum_{j=0}^{n-1} \omega_n^{-jk}Z_n^j
=& \sum_{j=0}^{n-1}\omega_n^{-jk} \sum_{i=0}^{n-1}\omega_n^{ij}\ket{i}\bra{i}
\\=& \sum_{i,j} \omega_n^{(i-k)j}\ket{i}\bra{i}
\\=& \sum_{i} \left(\sum_j \left(\omega_n^{(i-k)}\right)^j\right)\ket{i}\bra{i}
\\=& n\ket{k}\bra{k}
\end{align*}

We can then compose these matrices with powers of $X_n$ to derive any outer product $\ket{j}\bra{k}$ as a sum of matrices of the form $X_n^xZ_n^z$, but since these outer products are a basis of all $n\times n$ matrices, the $X_n^xZ_n^z$ matrices will form a basis as well, that is, \textit{any} $n\times n$ matrix will be a unique sum of such matrices. Since this basis is made of unitary matrices, it tends to be a very useful basis for reasoning about quantum algorithms.

\subsection{Pauli Group and Displacement Operators}

Matrices of the form $X_n^xZ_n^z$ will clearly sit inside the group generated by the Pauli matrices $X_n$ and $Z_n$. With the previous basis in mind we call this group the Pauli group. Observe that this group is not commutative, since $Z_nX_n = \omega_nX_nZ_n$:
\begin{align*}
Z_nX_n\ket{i}
=& Z_n\ket{i+1\mod n}
\\=& \omega_n^{i+1}\ket{i+1\mod n}
\\=& \omega_n^{i+1}X_n\ket{i}
\\=& \omega_nX_nZ_n\ket{i}
\end{align*}

From this it can also be seen that all elements of the Pauli group will be of the form $\omega_n^wX_n^xZ_n^z$, giving the group an order of $n^3$. Additionally, it can be shown that powers of such elements will behave in the following way:
\[(\omega_n^wX_n^xZ_n^z)^{\pm k} = \omega_n^{\pm kw+\frac{k(k \mp 1)xz}{2}}X_n^{\pm kx}Z_n^{\pm kz}\]

This is one motivation for rescaling the basis matrices $X_n^xZ_n^z$ to get the "Displacement" matrices $D_{x,z}$:
\[D_{x, z} = \omega_n^{\frac{xz}{2}}X_n^xZ_n^z = \omega_{2n}^{xz}X_n^xZ_n^z\]
These will have the property that $D_{x, z}^{\pm k} = D_{\pm kx, \pm kz}$. Equivalently they can be thought of as the geometric mean of $X_n^xZ_n^z$ and $Z_n^zX_n^x$, in that $D_{x,z}^2$ is equal to their product, essentially removing the ambiguous convention from order of generators.

Many of these displacement matrices will not actually appear in the Pauli group, which is motivation for defining the generalized Pauli group, or Weyl-Heisenberg group to be the group generated by these displacement operations, a group with order $2n^3$ and elements of the form $\omega_{2n}^tX_n^xZ_n^z = \omega_{2n}^{t'}D_{x, z}$.
